{
  "hash": "af09337f1d87cd9b220a49032a26b825",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Frequency attenuation effects in masked repetition priming: A large scale online lexical decision study\"\nformat:\n  tandf-pdf:\n    keep-tex: true\n    include-in-header:\n      text: |\n        \\usepackage{lscape}\n        \\newcommand{\\blandscape}{\\begin{landscape}}\n        \\newcommand{\\elandscape}{\\end{landscape}}  \n  tandf-html: default\nauthor:\n  - name: Roberto Petrosino\n    affiliations:\n      - ref: NYUAD\n    orcid: 0000-0002-8502-3070\n    email: roberto.petrosino@nyu.edu\n  - name: Diogo Almeida\n    affiliations:\n      - ref: NYUAD\n    orcid: 0000-0003-4674-8092\n    email: diogo@nyu.edu\naffiliations:\n  - id: NYUAD\n    name: New York University Abu Dhabi\n    department: Psychology Program, Division of Science\n    address: New York University Abu Dhabi\n    city: Abu Dhabi\n    country: United Arab Emirates\n    postal-code: 129188\nabstract: |\n    This study addresses the controversy surrounding the sensitivity of masked repetition priming to word frequency, a topic less contentious in unmasked priming. While unmasked priming exhibits a frequency attenuation effect, wherein high frequency words yield smaller repetition effects, this phenomenon has been inconsistently reported in masked priming. We conducted two large online experiments with rigorously validated frequency databases to reconcile past discrepancies. The first experiment confirmed masked repetition priming's viability in web browser-based settings. The pre-registered second study, designed for high statistical power, successfully identified a significant 9-ms frequency attenuation effect under masked priming conditions. This result indicates that the repetition effect in masked priming is less qualitatively distinct from unmasked priming than previously assumed, a finding with important practical and theoretical consequences. Moreover, our research underscores the usefulness of online masked priming experiments for detecting subtle effects that may elude traditional lab-based investigations.\nkeywords: \n  - masked repetition priming\n  - frequency attenuation effect\n  - online browser-based experiment\n  - power analysis\nbibliography: references.bib  \neditor: \n  markdown: \n    wrap: 72\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Introduction {#sec-intro}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(osfr)\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nhere() starts at /Users/rp3650/Library/CloudStorage/GoogleDrive-robpetrosino@gmail.com/My Drive/Academics/projects/morphology/morphological-decomposition/sub-projects/frequency-effects/freq_atten\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.3     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.4     v tibble    3.2.1\nv lubridate 1.9.3     v tidyr     1.3.0\nv purrr     1.0.2     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(knitr)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(rstatix)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'rstatix'\n\nThe following object is masked from 'package:stats':\n\n    filter\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(moderndive)\nlibrary(ggh4x)\nlibrary(ggpubr)\nlibrary(zoo)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n\n\n:::\n:::\n\n\n\n\nThe masked priming technique has been an invaluable tool in visual word recognition research. It has allowed researchers to study the conditions under which orthographic, phonological, morphological, and semantic information impact access to visual word forms while mitigating strategic effects and minimizing the influence of controlled processes [@Forster1998]. First introduced in its traditional form by @ForsterDavis1984 [see also @EvettHumphreys1981], this technique involves a forward mask (i.e., usually a string of hashes, #####), followed by a prime string presented for very short time ($SOA < 60$ ms),^[*SOA: Stimulus Onset Asynchrony*, i.e. the time between the start of one stimulus (in our case, the prime stimulus) and the start of another stimulus (the target stimulus). In the standard repetition priming design, no backward mask occurs between the prime and the target, and therefore SOA equals the duration of prime presentation.] and a target string presented immediately after. Because the prime presentation is so brief and masked by preceding and subsequent stimuli, most participants report not being aware that a prime string had been presented, and can at most report a screen flicker just before the target presentation [@ForsterEtal2003].\n\nAmong possible manipulations of prime-target relatedness, masked repetition priming (in which the same word is presented as both the prime and target within the same trial: e.g., *love-LOVE*) has been well studied, because its response seems to be qualitatively different from the unmasked counterpart ($SOA > 60 ms$): while high-frequency words benefit less from repetition than low-frequency words in the unmasked design [*frequency attenuation effect*, henceforth FAE\\; @ScarboroughEtal1977], this does not seem to be the case when the prime is masked [@ForsterDavis1984; @ForsterEtal1987; @SeguiGrainger1990; @Sereno1991; @ForsterDavis1991; @RajaramNeely1992; @BodnerMasson1997; @ForsterEtal2003; @Nievas2010].\n\nThis asymmetry in sensitivity to lexical frequency between the masked and unmasked repetition priming responses has been important in distinguishing among different models of priming in visual word recognition. More specifically, _interactive activation models_ [@McClellandRumelhart1981; @GraingerJacobs1996; @ColtheartEtal2001] conceive of priming as a \"head start\" in processing due to the pre-activation of the target word due to the presentation of the prime. According to this type of model, priming is ultimately due to this single mechanism, and therefore it is difficult to reconcile interactive activation models with the qualitatively different profiles for repetition priming in masked and unmasked conditions.\n\nSimilarly, episodic models [e.g., @Jacoby1981; @Jacoby1983] posit a different single mechanism for priming effects, that is, the activation/retrieval of the episodic memory trace of the encounter with the prime word. These models therefore encounter the same type of difficulty in accounting for qualitatively different patterns of repetition priming effects in masked and unmasked conditions. A similar type of model, called the _memory recruitment model_ makes very similar predictions to the episodic memory models, positing a non-lexical source for priming effects [@BodnerMasson1997; @MassonBodner2003; @Bodner2014]. Repetition priming effects under this view stem from the exploitation, strategically or automatically, of a memory resource created by the encounter with the prime word. The frequency attenuation effect, under episodic and memory recruitment models alike, is predicted on the basis that low frequency primes, being more distinctive stimuli, create a more potent and effective memory resource compared to high frequency primes.\n\nAlternatively, other models with a more fine-grained view of the lexical access process seem to sidestep the problem posed by the qualitatively different repetition priming profiles observed in masked and unmasked conditions. One such model is the _entry-opening model_ [also known as the bin model\\; @ForsterDavis1984]. According to this model, when the visual stimulus is presented, lexical entries are assigned to specific bins based on orthographic similarity. In the first stage (fast search stage), a fast, frequency-ordered search goes through the entries within a given bin, and compares each one with the the input stimulus, thus assigning to each entry a goodness-of-fit score. This comparison is fast and crude, and sorts entries into (a) perfect (i.e., no difference is detected between the input and the entry), (b) close (i.e., small differences are detected), and (c) irrelevant matches (i.e., substantial difference are detected). Any entry of type (a) or (b) is opened, so that the entry can be further analyzed and compared to the input in the subsequent verification stage. In the masked environment, the entry of the prime word is opened at the fast search stage, but its short presentation duration prevents it from reaching the evaluation stage, but the entry is nonetheless left open. Upon the presentation of the target, the access procedure will follow its two stage course, with a frequency-sensitive fast search and a subsequent entry opening for evaluation/verification. In this view, the fast search for the target word proceeds normally, but the evaluation/verification procedure starts and ends sooner than it otherwise would, because the target entry has already been left open after the brief processing of the prime. Thus, the _entry-opening model_ explains the masked repetition priming as the benefit from having the entry of the target word already open when it is going to be evaluated in the second stage of recognition. Crucially, this occurs *after* the target word is initially accessed, which happens in order of frequency. Put differently, according to the _entry-opening model_, masked repetition priming occurs because of the time savings from not having to open the entry, which is a frequency-insensitive process (every entry takes the same time to be opened), but *after* the frequency-sensitive first access stage. As a consequence, _entry-opening model_ predicts a frequency-insensitive masked repetition priming effect, which is what has been traditionally reported in the literature (see @tbl-litReview). In addition, it also (correctly) predicts that pseudowords should not benefit from masked repetition priming, as they have no entries in the mental lexicon to be left open after the brief processing of the prime.\n\n```{.content-hidden}\nNOTE: DECIDED TO REMOVE DISCUSSION OF BAYESIAN READER. This is basically because the predictions of this model are really unclear for the FAE. On the one hand, the model claims that masked priming occurs because the prime and target are treated as the same event, which predicts simply a frequency effect, but no interaction with repetition. However, Norris & Kinoshita (2008) do report the FAE, and it seems that their simulations of the model do predict a small FAE. However, they never explain this, or make this prediction explicit. They only say that they replicated the results from Kinoshita 2006 and leave it at that. In their 2018 EEG paper, they do not obtain an FAE behaviorally, but they seem indifferent to it. In none of the papers from Bayesian reader it seems that the FAE is actually discussed. The closest we get is in one paper I need to remember where they claim that a mix of high and low frequency words will create underestimates for the priors of frequent words and overestimates for priors of low frequency words. We can always add all these caveats if a reviewer wants it.\n##-----\nFinally, a more recent model called the Bayesian Reader (CITATION: Norris, 2006) conceives of visual word perception as a Bayesian inference problem, and is thus highly sensitive to task demands, as different tasks generate different perceptual hypotheses to be tested. When it comes to the frequency attenuation effect in masked priming, the predictions of the model are a little unclear. On the one hand, the Bayesian Reader model posits that under masked priming, the perceptual system is \"tricked\" into treating the prime and target presentations as a single event.  \n```\n\n```{.content-hidden}\nis not easy to explain, as it seems to hint at two different mechanisms that are activated accordingly. In persisting activation models [@McClellandRumelhart1981; @GraingerJacobs1996; @ColtheartEtal2001], priming is seen as the \"head start\" effect that the prior presentation of the prime gives to the activation of the target. Therefore, regardless of the prime-target SOA, the priming magnitude is expected to be either inversely proportional to word frequency, especially if word frequency is encoded in terms of changes in connection strengths (or activation thresholds). In memory recruitment models [a.o., @BodnerMasson1997; @MassonBodner2003], priming is seen as the effect whereby the prior recruitment of the memory representation of the prime assist with the identification of the memory representation of the target. Similar to persistent activation models, memory recruitment models predict that, whatever the SOA is, the priming magnitude and word frequency inversely interact, since word frequency is encoded as episodic distinctiveness. As the episodic representations of low-frequency words are more distinctive (because they are by definition less heard and used), they would trigger a greater response than the high-frequency words. In contrast, in \n```\n\nHowever, as @tbl-litReview shows, there are nonetheless a few studies that do report significant FAEs in masked repetition priming [@BodnerMasson2001; @Kinoshita2006; @NorrisKinoshita2008]. @BodnerMasson2001 report that when stimuli are presented in alternating case (e.g., _pHoNe_), this increases the lexical decision difficulty and therefore creates an extra incentive to draw on the memory resource created by the brief processing of the prime. Under such conditions, they were able to observe a statistically significant FAE. In the same vein, @Kinoshita2006 noticed that in earlier studies the low frequency words often had very high error rates, and suggested that perhaps many participants did not know them. If participants treated a substantial number of low frequency words as nonwords, and nonwords do not exhibit repetition priming under masked conditions, it could artificially depress the repetition priming effect. In two separate experiments, @Kinoshita2006 showed that larger repetition priming effects for low frequency words were only obtained when the low frequency words were vetted to make sure the participants knew them prior to the experiment. Following up on @Kinoshita2006, @NorrisKinoshita2008 were also able to find an interaction between lexical frequency and repetition in masked repetition priming.\n\nFinally, as @tbl-litReview shows, it is noteworthy that 15 out of 18 previous studies showed numerically larger masked priming effects for low frequency words as opposed to high frequency words, irrespective of statistical significance. Similarly, the average repetition effect for low frequency words in the studies reviewed in @tbl-litReview is 13 ms larger when compared to that of high frequency words. These results are not in line with the predictions dictated by the _entry opening model_, and seem to align better with the predictions made by _interactive activation models_ and _memory recruitment_ models.\n\n\\blandscape\n\n\n\n::: {#tbl-litReview .cell tbl-cap='Summary of the masked repetition priming effects as a function of word frequency reported in the literature. The power range estimates were calculated by simulating 10,000 datasets with the corresponding sample size (N) and FAE = 15 ms and 30 ms.'}\n\n```{.r .cell-code .hidden}\nlit_effects_tbl_filename <- \"supplemental-data/MaskedPrimingDatabase-IDPriming-InteractionFrequency.csv\"\n\nlit_effects <- read_csv(lit_effects_tbl_filename)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 18 Columns: 8\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (4): PAPER, LANGUAGE, PRIME_DURATION, p<.05?\ndbl (4): N, MOP_HF, MOP_LF, MOP_Interaction\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\npower_estimates_15 <- read_csv(\"supplemental-data/power_estimate2.dataset.range.csv\") %>%\n  select(nsubj, minPower, maxPower) %>%\n  mutate(across(minPower:maxPower, ~round(., 2))) %>%\n  mutate(minPower = paste0(\"[\", minPower), maxPower = paste0(maxPower, \"]\")) %>%\n  unite(\"Power range\", minPower:maxPower, sep = \" \")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 17 Columns: 11\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (3): file_name, PAPER, EXP.\ndbl (8): nsubj, minPower, maxPower, rho_min, std_dev_min, rho_max, std_dev_m...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\npower_estimates_30 <- read_csv(\"supplemental-data/power_estimate3.dataset.range.csv\") %>%\n  select(nsubj, minPower, maxPower) %>%\n  mutate(across(minPower:maxPower, ~round(., 2))) %>%\n  mutate(minPower = paste0(\"[\", minPower), maxPower = paste0(maxPower, \"]\")) %>%\n  unite(\"Power range\", minPower:maxPower, sep = \" \")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 17 Columns: 11\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (3): file_name, PAPER, EXP.\ndbl (8): nsubj, minPower, maxPower, rho_min, std_dev_min, rho_max, std_dev_m...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlit_effects %>%\n  left_join(., power_estimates_15, by=join_by(\"N\" == \"nsubj\"), multiple=\"first\") %>%\n  left_join(., power_estimates_30, by=join_by(\"N\" == \"nsubj\"), multiple=\"first\", suffix = c(\"_15\", \"_30\")) %>%\n  mutate(across(MOP_HF:MOP_Interaction, as.numeric)) %>%\n  #group_by(N) %>%\n  gt(rowname_col = \"PAPER\") %>%\n  #tab_options(row.striping.include_table_body = FALSE) %>% \n  tab_stubhead(label = \"Study\") %>%\n  tab_spanner(\n    label = \"MOP (ms)\", columns = c(MOP_HF, MOP_LF)\n  ) %>% \n  tab_spanner(\n    label = \"FAE (ms)\", columns = c(MOP_Interaction, `p<.05?`)\n  ) %>%\n  tab_spanner(\n    label = \"Power range [min max]\", columns = c(`Power range_15`, `Power range_30`)\n  ) %>%\n  cols_label( LANGUAGE = \"Language\", \n              PRIME_DURATION = \"SOA\",\n              MOP_HF = \"HF\", \n              MOP_LF = \"LF\", \n              MOP_Interaction = \"ES\",\n              `p<.05?` = md(\"_p_<.05?\"),\n              `Power range_15` = \"FAE=15ms\",\n              `Power range_30` = \"FAE=30ms\"\n  ) %>%\n  tab_footnote( \n    footnote = \"SOA for each subject determined by pre-test\", \n    locations = cells_body(column = \"PRIME_DURATION\", rows = 14)\n  ) %>%\n  tab_footnote(\n    footnote = \"Reported in Masson & Bodner (2003)\",\n    locations = cells_stub(rows = 16) \n  ) %>% \n  grand_summary_rows(\n    columns = c(MOP_HF, MOP_LF, MOP_Interaction),\n    fns = list(Mean ~ round(mean(.)),\n               SD ~ round(sd(.))), \n    missing_text = \" \" \n  ) %>%\n  grand_summary_rows(\n    columns = c(MOP_Interaction),\n    fns = list(Correlation ~ round(cor(MOP_HF, MOP_LF), 2)), \n    missing_text = \" \" \n  ) %>%\n   sub_missing(\n    missing_text = \" \"\n  )\n```\n\n::: {.cell-output-display}\n\\setlength{\\LTpost}{0mm}\n\\begin{longtable}{l|lrlrrrrll}\n\\toprule\n\\multicolumn{1}{l}{} &  &  &  & \\multicolumn{2}{c}{MOP (ms)} & \\multicolumn{2}{c}{FAE (ms)} & \\multicolumn{2}{c}{Power range [min max]} \\\\ \n\\cmidrule(lr){5-6} \\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\n\\multicolumn{1}{l}{Study} & Language & N & SOA & HF & LF & ES & \\emph{p}\\textless{}.05? & FAE=15ms & FAE=30ms \\\\ \n\\midrule\\addlinespace[2.5pt]\nForster, Davis, Schoknecht, \\& Carter (1987), exp. 1 & English & 16 & 60 & 61 & 66 & 5 &   & [0.02 0.24] & [0.04 0.84] \\\\ \nNorris, Kinoshita, Hall, \\& Henson (2018) & English & 16 & 50 & 38 & 51 & 13 &   & [0.02 0.24] & [0.04 0.84] \\\\ \nSereno (1991), exp. 1 & English & 20 & 60 & 40 & 64 & 24 &   & [0.02 0.33] & [0.04 0.92] \\\\ \nForster \\& Davis (1991), exp. 5 & English & 24 & 60 & 54 & 72 & 18 &   & [0.02 0.4] & [0.05 0.96] \\\\ \nBodner \\& Masson (1997), exp. 1 & English & 24 & 60 & 29 & 45 & 16 &   & [0.02 0.4] & [0.05 0.96] \\\\ \nBodner \\& Masson (1997), exp. 3 & English & 24 & 60 & 36 & 50 & 14 &   & [0.02 0.4] & [0.05 0.96] \\\\ \nForster, Mohan, \\& Hector (2003), exp. 1 & English & 24 & 60 & 63 & 60 & -3 &   & [0.02 0.4] & [0.05 0.96] \\\\ \nKinoshita (2006), exp. 1 & English & 24 & 53 & 32 & 38 & 6 &   & [0.02 0.4] & [0.05 0.96] \\\\ \nKinoshita (2006), exp. 2 & English & 24 & 53 & 29 & 59 & 30 & * & [0.02 0.4] & [0.05 0.96] \\\\ \nNorris \\& Kinoshita (2008), exp. 1 & English & 24 & 53 & 35 & 66 & 31 & * & [0.02 0.4] & [0.05 0.96] \\\\ \nForster, Davis, Schoknecht, \\& Carter (1987), exp. 4 & English & 27 & 60 & 34 & 25 & -9 &   & [0.03 0.46] & [0.05 0.98] \\\\ \nForster \\& Davis (1984), exp. 1 & English & 28 & 60 & 45 & 38 & -7 &   & [0.03 0.48] & [0.06 0.98] \\\\ \nNievas (2010), exp. 1b & Spanish & 30 & 50 & 44 & 65 & 21 & * & [0.03 0.52] & [0.06 0.99] \\\\ \nNievas (2010), exp. 2a & Spanish & 30 & 50 or 33\\textsuperscript{\\textit{1}} & 51 & 58 & 7 &   & [0.03 0.52] & [0.06 0.99] \\\\ \nSegui \\& Grainger (1990), exp. 4 & French & 36 & 60 & 42 & 45 & 3 &   & [0.03 0.63] & [0.07 1] \\\\ \nBodner \\& Masson (2001), exps. 2A, 2B, 3, \\& 6 (average)\\textsuperscript{\\textit{2}} & English & 40 & 60 & 37 & 69 & 32 & * & [0.03 0.68] & [0.08 1] \\\\ \nRajaram \\& Neely (1992), exp. 1 & English & 48 & 50 & 30 & 37 & 7 &   & [0.04 0.76] & [0.09 1] \\\\ \nRajaram \\& Neely (1992), exp. 2 & English & 48 & 50 & 45 & 78 & 33 &   & [0.04 0.76] & [0.09 1] \\\\ \n\\midrule \n\\midrule \nMean &   &   &   & 41 & 55 & 13 &   &   &   \\\\ \nSD &   &   &   & 10 & 14 & 13 &   &   &   \\\\ \nCorrelation &   &   &   &   &   & 0.46 &   &   &   \\\\ \n\\bottomrule\n\\end{longtable}\n\\begin{minipage}{\\linewidth}\n\\textsuperscript{\\textit{1}}SOA for each subject determined by pre-test\\\\\n\\textsuperscript{\\textit{2}}Reported in Masson \\& Bodner (2003)\\\\\n\\end{minipage}\n\n:::\n\n```{.r .cell-code .hidden}\nlitReview.stats <- lit_effects %>%\n    select(PAPER, MOP_HF, MOP_LF) %>%\n    pivot_longer(MOP_HF:MOP_LF, names_to = \"frequency.condition\", values_to=\"mean.priming\") %>%\n    mutate(frequency.condition = factor(frequency.condition, levels=c(\"MOP_LF\", \"MOP_HF\"))) %>%\n    t_test(mean.priming ~ frequency.condition, paired=T, detailed=T)\n```\n:::\n\n\n\n\\elandscape\n\n# The present study {#sec-study}\n\nIt is somewhat surprising that the status of the FAE in masked priming remains largely unresolved in the literature, given its substantial magnitude (especially considering that the difference between two conditions seems to be statistically significant: *M_FAE* = 13, CI_95% = [7, 20]), *t*(17) = 4.24, $p=.0005$), and its theoretical significance in elucidating the underlying cognitive processes of masked priming.\n\nOne possible interpretation of the conflicting past findings revolves around the fact that only 4 out of 18 studies demonstrate a statistically significant FAE in masked repetition priming. Notably, this number potentially diminishes further when considering that, among these four studies, the FAE is detected only through the pooling of data across multiple studies employing a unique alternating-case stimulus presentation [@BodnerMasson2001; @MassonBodner2003]. This line of reasoning suggests a qualitatively distinct profile between masked and unmasked repetition priming, with the FAE more firmly established in the latter.\n\nConversely, one could argue that 15 out of 18 studies exhibit numerically larger repetition effect sizes for low-frequency words compared to high-frequency words â€”- a pattern that is challenging to reconcile with a genuine absence of interaction between frequency and masked repetition. Additionally, the average FAE across all studies stands at 13 ms, a modest yet non-negligible (and statistically significant) effect size. These considerations suggest that a genuine FAE may exist in masked priming but might be smaller than the thresholds detectable by most previous experiments.\n\nCompounding this complexity, another potential contributor to past discrepancies is the reliance on the dated Kucera and Francis (1967) word frequency database, which 15 out of 18 studies have depended on. This poses a potential problem, as this frequency database has consistently demonstrated inferior predictive performance, particularly with low-frequency words, compared to more contemporary databases [@Burgess1998; @Zevin2002; @Balota2004; @BrysbaertNew2009; @Yap2009; @Brysbaert2011; @Gimenes2016; @Herdaugdelen2017; @Brysbaert2018]. Both of these issues are addressed in the subsequent sections.\n\n## Issues with frequency databases {#sec-study-freq}\n\nDue to the well-documented concerns over the reliability of the @KuceraFrancis1967 frequency database for psycholinguistic experiments [@Burgess1998; @Zevin2002; @Balota2004; @BrysbaertNew2009; @Yap2009; @Brysbaert2011; @Gimenes2016; @Herdaugdelen2017; @Brysbaert2018], our studies exclusively sourced materials from the HAL [@LundBurgess1996] and SUBTLEX${US}$ [@BrysbaertNew2009] databases, which reflect more recent linguistic usage and offer better validation in behavioral experiments [e.g., @Balota2004; @BrysbaertNew2009; @Yap2009; @Brysbaert2011; @Gimenes2016; @Herdaugdelen2017]. While these databases outperform @KuceraFrancis1967 in predicting psycholinguistic task outcomes, it is important to note potential discrepancies in individual frequency counts, particularly in the low and mid-frequency ranges. This variation, attributable to the primary genre of their sources (USENET groups for HAL and movie subtitles for SUBTLEX$_{US}$),[^frequency-databases] may have minimal impact on megastudies with large word samples [e.g., @Balota2004; @BrysbaertNew2009; @Yap2009; @Brysbaert2011; @Gimenes2016; @Herdaugdelen2017]. However, corpus-specific frequency skew can become significant when dealing with smaller samples of words in studies such as masked priming. @tbl-exFreqSkew illustrates the potential discrepancy in considering words as high or low frequency based on the different aforementioned databases.\n\n[^frequency-databases]: A separate, though relevant issue which cannot be addressed here is to how to mitigate the discrepancies across the databases available, but see @Yap2009, and @Brysbaert2011 for proposals about combining the frequency counts from different corpora.\\label{fn-databases}\n\n\n\n\n::: {#tbl-exFreqSkew .cell tbl-cap='Example of frequency count imbalances (in occurrences per million) across the frequency norms of Kucera & Francis (KF), HAL and SUBTLEX~US~ for 4 to 6 letter words.'}\n\n```{.r .cell-code .hidden}\nfreqSkew <- list.files(path=\"supplemental-data/\", pattern=\"_skew_\", full.names = T) %>%\n  lapply(function(x) read_csv(x)) %>% bind_rows()\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 5 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Word\ndbl (3): KF, HAL, SUBTLEX-US\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 5 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Word\ndbl (3): KF, HAL, SUBTLEX-US\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 5 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Word\ndbl (3): KF, HAL, SUBTLEX-US\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nfreqSkew %>% \n  gt() %>%\n  cols_label(\n    `SUBTLEX-US` = md(\"SUBTLEX~US~\")\n  ) %>%\n  fmt_number(decimals = 0, drop_trailing_zeros = TRUE) %>%\n  tab_row_group(\n    label = md(\"_Skew in KF_\"),\n    rows = 1:5\n  ) %>%\n  tab_row_group(\n    label = md(\"_Skew in HAL_\"),\n    rows = 6:10\n  ) %>%\n  tab_row_group(\n    label = md(\"_Skew in SUBTLEX~US~_\"),\n    rows = 11:15\n  ) %>%\n  row_group_order(groups = c(\"_Skew in KF_\", \"_Skew in HAL_\", \"_Skew in SUBTLEX~US~_\"))\n```\n\n::: {.cell-output-display}\n\\begin{longtable}{lrrr}\n\\toprule\nWord & KF & HAL & SUBTLEX\\textsubscript{US} \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{4}{l}{\\emph{Skew in KF}} \\\\ \n\\midrule\\addlinespace[2.5pt]\nnegro & $104$ & $3$ & $5$ \\\\ \npoet & $99$ & $9$ & $9$ \\\\ \nmercer & $71$ & $4$ & $2$ \\\\ \nswung & $48$ & $3$ & $2$ \\\\ \nmantle & $48$ & $8$ & $2$ \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{4}{l}{\\emph{Skew in HAL}} \\\\ \n\\midrule\\addlinespace[2.5pt]\nweb & $6$ & $351$ & $9$ \\\\ \nuser & $4$ & $297$ & $2$ \\\\ \nmint & $7$ & $211$ & $5$ \\\\ \nformat & $9$ & $198$ & $1$ \\\\ \nwarp & $4$ & $125$ & $5$ \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{4}{l}{\\emph{Skew in SUBTLEX\\textsubscript{US}}} \\\\ \n\\midrule\\addlinespace[2.5pt]\ndaddy & $4$ & $16$ & $185$ \\\\ \nbitch & $6$ & $24$ & $169$ \\\\ \ncute & $5$ & $28$ & $88$ \\\\ \npardon & $8$ & $12$ & $65$ \\\\ \nsteal & $5$ & $28$ & $53$ \\\\ \n\\bottomrule\n\\end{longtable}\n\n:::\n:::\n\n\n\n\n## Issues with statistical power {#sec-study-power}\n\nThe inconsistency of past findings regarding the FAE in masked priming has been linked to a potential lack of statistical power in previous research [@BodnerMasson1997; @BodnerMasson2001; @MassonBodner2003]. This is a plausible concern, as interactions like the FAE often require larger sample sizes for statistical detection [@PotvinSchtuz2000; @BrysbaertStevens2018] compared to main effects. We outline below three ways in which neglecting statistical power might frustrate our understanding of FAE in masked repetition priming.\n\nFirst, our literature review revealed crucial gaps in reporting relevant statistical information, obstructing the assessment of the statistical power attained by past experiments. The inconsistent reporting of each conditions' standard deviations (only 7 out of 18 studies) and the complete lack of reporting of the correlation structure between conditions complicates power assessments. Researchers are thus forced to explore a range of plausible values for standard deviations and correlation structures on their own.\n\n@tbl-litReview details our attempt to conduct power simulations for two hypothesized frequency attenuation effect sizes: 15 ms, close to the averaged FAE of 13 ms, and 30 ms, close to the only four observed statistically significant FAE. Standard deviations (ranging between 60 ms and 180 ms, in 10 ms increments) and correlation between conditions (uniformly set to range between 0.6 and 0.9, with 0.1 unit increments) were simulated for each study's sample size, with 10,000 replications for each simulation. These range of values were derived from our literature review and previous in lab and online experiments [@Petrosino2020; @PetrosinoEtal2023]. For each simulated dataset, a paired _t_-test was performed comparing the repetition effect for high frequency words and low frequency words. This calculation is mathematically identical to the interaction term in a 2x2 factorial within-subjects design, but it is less computationally expensive to perform in large scale simulations. Power to detect this interaction was then calculated as the proportion of significant tests obtained across replications. All else being equal, standard deviations and correlations between conditions have opposite effects on statistical power: increases in standard deviations lead to less power, while increases in correlation between conditions lead to more power.\n\nThe results reported in @tbl-litReview reveal a wide range of possible statistical power attained by previous studies, depending solely on the combination of plausible standard deviation and correlation across conditions. For instance, the study with the smallest sample size [@ForsterEtal1987, $N=16$] had a 2% to 24% chance of detecting a 15 ms frequency attenuation effect and a 4% to 84% chance for a 30 ms effect. Similarly, the study with the largest sample size [@RajaramNeely1992, $N=48$] exhibited a range of 4% to 76% for a 15 ms frequency attenuation effect and 9% to 100% for a 30 ms effect. As a consequence of the limited reporting of relevant statistical information in past studies, it is nearly impossible to determine if any of them were adequately powered to detect the effect of interest.\n\nA second concern arising from the ambiguity surrounding statistical power in the literature is the potential impact of a prevalence of low-powered experiments on the scientific record. An excess of such experiments increases the risk of observed statistically significant effects being spurious [@Button2013]. As highlighted in @tbl-litReview, only 4 out of 18 studies demonstrate a statistically significant FAE. The absence of clarity regarding the statistical power of previous research poses challenges in assessing the likelihood of these significant findings being spurious.\n\nFinally, it is widely acknowledged that experiments with approximately 50% power are akin to a coin toss in their ability to detect a true effect [@Cohen1992]. A less-appreciated fact is that, in the presence of even lower power (<25%), statistically significant results can substantially overestimate the effect size -- a type-M error [@GelmanCarlin2014]. When power drops to levels below 10%, a statistically significant result may occur even when the observed effect goes in the opposite direction of the true effect -- a type-S error [@GelmanCarlin2014]. Our power simulations for within-subjects data revealed a similar relationship between statistical power, type-M, and type-S errors in line with the observations detailed by @GelmanCarlin2014 for the independent samples $t$-test. For instance, at 10% power (a possibility for virtually all previous studies, as indicated in @tbl-litReview), a statistically significant result could indicate an overestimation of the magnitude of the frequency attenuation effect by a factor between 2 and 5, with a 5% chance of incorrectly determining the direction of the effect.\n\n\n\n\n::: {#cell-fig-power .cell}\n\n```{.r .cell-code .hidden}\nload(\"supplemental-data/freq_atten_exp2.10ms.RData\")\n\nfreq_atten_exp2.10ms_df.sub <- freq_atten_exp2.10ms_df %>% \n  filter(ES == 10) %>%\n  filter(std_dev >= 80 & std_dev <= 120) %>%\n  filter(rho >= 0.7 & rho <= 0.9)\n\nfreq_atten_exp2.10ms_df.sub |>\n  ggplot(aes(y = power_unadjusted, x = nsubj)) + \n    geom_line() + \n    geom_point() +\n    geom_hline(yintercept = 0.8, color = \"red2\") + \n    facet_grid(rho ~ std_dev) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = \"sd\", breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = \"cor\", breaks = NULL, labels = NULL)) +\n  theme_bw()+\n    theme(#axis.line = element_line(colour = \"black\"),\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(), \n      panel.border = element_blank(),\n      legend.position=\"none\"\n      #panel.background = element_blank() \n     )+\n  labs(title=\"FAE = 10 ms\", y = \"power (adjusted)\", x = \"sample size\")\n```\n\n::: {.cell-output-display}\n![Power simulations for a FAE = 10 ms, for all combinations of standard deviation (sd), correlation (cor), and sample size. The red line identifies the threshold of 80% power.](index_files/figure-pdf/fig-power-1.pdf){#fig-power fig-pos='H'}\n:::\n:::\n\n\n\n\nThe two studies reported here were designed to mitigate these two confounding issues: the overreliance on the @KuceraFrancis1967 frequency data as well as a potential lack of statistical power observed in previous research. As a large increase in statistical power requires a large sample size, Experiment 1 aimed to assess the suitability of using *Labvanced* [@Labvanced2017], an online platform for running web browser-based experiments, for running masked priming studies online.\n\n# Experiment 1 {#sec-exp1}\n\nAs evident in @tbl-litReview, conducting a properly powered experiment for a FAE close to the averaged value calculated from previous studies will require sample sizes that would be impractical to pursue in standard university research settings, typically quiet lab rooms with limited research computers. In response to this challenge, our study was exclusively conducted online, leveraging the growing trend in online behavioral research facilitated by HTML5 capabilities and the availability of advanced web software such as *jsPsych* [@deLeeuw2014], *PsychoJS* (the JavaScript counterpart of *PsychoPy*, @PeirceEtal2019), *Gorilla* [@Anwyl2020], and *Labvanced* [@Labvanced2017].\n\nNotably, two recent studies have already demonstrated the viability of conducting masked priming experiments online, employing different software tools: @Angele2023 with *PsychoJS*, and @Cayado2023 with *Gorilla*. In our study, we opted for *Labvanced* [@Labvanced2017]. This choice was motivated by our university's recent acquisition of a group license for *Labvanced*, itself motivated by its user-friendly GUI-based web app nature. Similar to *Gorilla*, *Labvanced* eliminates local installation issues, ensuring cross-platform consistency and simplifying experimental design without necessitating proficiency in additional programming languages.\n\n## Methods {#sec-exp1-methods}\n\n### Materials {#sec-exp1-methods-materials}\n\nTwo hundred five-letter English words were selected from the English Lexicon Project [ELP\\; @balota2007], in which 100 words were selected from an upper and a lower frequency range, respectively.[^low-freq] It was not possible to identify two frequency ranges that were well separated from one another for both the HAL [@LundBurgess1996] and the SUBTLEX$_{US}$ [@BrysbaertNew2009] frequency databases. As @tbl-words_exp1 shows, we managed to do this only for the former, whereas some overlap was present in the latter, as expected given the different source of the two databases (see above, and fn. \\ref{fn-databases}). The two word subsets corresponded to the two word frequency conditions being tested: the high-frequency, and low-frequency conditions. In each condition, fifty words were randomly chosen to be presented as targets and related primes (for the related prime type condition), and the remaining fifty were presented as unrelated primes (for the unrelated prime type condition).\n\n[^low-freq]: The experiment also included an even lower frequency condition (range: [3.0 5.01]; mean: 4.39, SD: 0.50), thus summing up to six hundred trials being presented in the experiment. However, the average error rate for this condition was 44% and 33 (out of the 50) target words used in the same condition had a error rate higher than 30%. This suggested that they might have not known these words at all (see @Kinoshita2006). For this reason, this condition was completely removed from analysis and will not be mentioned in the rest of this article.\n\n\n\n\n\n::: {#tbl-words_exp1 .cell tbl-cap='Experiment 1. Descriptive statistics of the word item used. For both frequency databases, the word frequencies were converted to per-million count to ensure cross-comparison.'}\n\n```{.r .cell-code .hidden}\nexp1_wordlists_folder <- \"./materials/experiment1/\"\n\nstimuli <- read.csv(paste0(exp1_wordlists_folder, \"frequency-effects_experiment1_stimuli.csv\"))\ndatabase <- read.csv(paste0(exp1_wordlists_folder, \"stim-database.csv\"))\n\nstimuli <- merge(stimuli, database, by='Word', all.x=T, all.y=F)\nstimuli <- stimuli[c(-559,-596),] #one word and one non-word were duplicated as being considered both non-words and words\n\nwords <- stimuli %>% filter(type != 'non-word') \nnonwords <- stimuli %>% filter(type == 'non-word')\n\nwords$freq.bin <- factor(words$freq.bin, levels = c(\"high\", \"mid\"))\nwords$type <- as.factor(words$type)\n\nwords %>% \n  filter(freq.bin != 'low') %>%\n  mutate(Freq_HAL.Pm = (Freq_HAL * (10^6) / (131*10^6))) %>% # CHECK IF THE TRANSFORM HERE IS CORRECT\n  mutate(freq.bin = fct_recode(freq.bin, low = \"mid\")) %>% \n  dplyr::group_by(freq.bin) %>%\n  dplyr::summarise(N = n(),\n            minFreq= min(Freq_HAL.Pm, na.rm = T), maxFreq=max(Freq_HAL.Pm, na.rm = T),\n            meanFreq = mean(Freq_HAL.Pm, na.rm = T), sdFreq = sd(Freq_HAL.Pm, na.rm = T),\n            minSUBFreq= min(SUBTLWF, na.rm = T), maxSUBFreq=max(SUBTLWF, na.rm = T),\n            meanSUBFreq = mean(SUBTLWF, na.rm = T), sdSUBFreq = sd(SUBTLWF, na.rm = T),\n            #meanRT=mean(as.numeric(I_Mean_RT), na.rm = T), sdRT=sd(as.numeric(I_Mean_RT), na.rm = T),\n            #meanLength = mean(Length, na.rm = T), sdLength=sd(Length, na.rm = T)\n            ) %>% \n  mutate(\n    across(-freq.bin, ~ if_else(.<1, round(., 2), round(.)))\n  ) %>% \n  gt() %>%\n  tab_spanner(\n    label = md(\"**HAL**\"), \n    columns = c(minFreq: sdFreq)\n  ) %>%\n  tab_spanner(\n    label = md(\"**SUBTLEX~US~**\"), \n    columns = c(minSUBFreq: sdSUBFreq)\n  ) %>%\n  cols_label(freq.bin = \"frequency\", \n             minFreq = \"min\", maxFreq = \"max\", meanFreq = \"mean\", sdFreq = \"SD\", \n             minSUBFreq = \"min\", maxSUBFreq = \"max\", meanSUBFreq = \"mean\", sdSUBFreq = \"SD\") %>%\n  gt_add_divider(columns=c(\"maxFreq\", \"maxSUBFreq\"), weight=px(1))\n```\n\n::: {.cell-output-display}\n\\begin{longtable}{crrrrrrrrr}\n\\toprule\n &  & \\multicolumn{4}{c}{\\textbf{HAL}} & \\multicolumn{4}{c}{\\textbf{SUBTLEX\\textsubscript{US}}} \\\\ \n\\cmidrule(lr){3-6} \\cmidrule(lr){7-10}\nfrequency & N & min & max & mean & SD & min & max & mean & SD \\\\ \n\\midrule\\addlinespace[2.5pt]\nhigh & 100 & 169 & 1212 & 482 & 292 & 2.00 & 1168 & 129 & 201 \\\\ \nlow & 100 & 3 & 23 & 9 & 5 & 0.12 & 13 & 3 & 3 \\\\ \n\\bottomrule\n\\end{longtable}\n\n:::\n:::\n\n\n\n\nTwo-hundred five-letter phonotactically legal nonwords were randomly selected from the ELP database as well. Half of them were randomly selected to be presented as targets; the other half was instead used as unrelated nonword primes.\n\n###  Participants {#sec-exp1-methods-participants}\n\nThree hundred participants (145 females; age mean: 38.48; age sd: 12.44) were recruited on Prolific (<https://www.prolific.com>). Several criteria were selected to ensure recruitment of native speakers of U.S. English. Participants needed to be born in the Unites States of America, speak English as their first and only language, and have no language-related disorder. We encouraged participants to avoid any sort of distraction throughout the experiment, and to close any program that may be running in the background, as a way to boost performance of the stimulus presentation in the web browser as much as possible. Because the experiment was run online, participants could not be monitored in any way during data collection. Finally, to further reduce variability across participants' devices, we restricted the experiment to be run on Google Chrome only, which is the most used browser worldwide (<https://www.w3counter.com/globalstats.php>), and reportedly performs better than any other across operating systems [likely thanks to the Blink engine\\; see @LukacsGaspar2023].\n\n### Procedure {#sec-exp1-methods-proc}\n\nEach recruited participant was assigned one of two word lists, which differed only in the relatedness of the prime with respect to the target; otherwise, the two lists presented the same set of target words and nonwords (300 items in total). In one list, the three conditions (high-frequency, low-frequency word conditions, and the non-word condition) had 25 target items being preceded by themselves (the *related* condition) and the remaining 25 target items being preceded by one of the unrelated primes belonging to the same frequency bin (the *unrelated* condition). In the other list, these assignments were reversed. The order of stimulus presentation was randomized for each participant.\n\nAfter being recruited, participants were asked to click on a link which redirected them to Labvanced. During the experiment, they were asked to perform a lexical decision task by pressing either the 'J' (for word) or 'F' (for non-word) keys on their keyboard. Each trial consisted of three different stimuli appearing at the center of the screen: a series of hashes (#####) presented for 500 ms, followed by a prime word presented for 33 ms, and finally the target word; the target word disappeared from the screen as soon as a decision was made. The motivation for the choice of a very short prime duration (as compared to the literature, in which it is usually between 50 and 60 ms; see @tbl-litReview) is threefold. First, previous pilot experiments on *Labvanced* showed that, due to the inherent difficulties in presenting stimuli for very short set durations on the browser, a longer duration would increase the number of trials in which the prime duration would rise above the subliminal threshold (usually set at 60 ms) due to timing inaccuracies and missing screen refreshes, which could trigger the adoption of experiment-wide strategies in the task, and ultimately contaminate the masked priming response. Second, @Angele2023 and @Cayado2023 have demonstrated that a 33 ms priming duration does elicit repetition priming effects in online experiments. Finally, setting such a short prime duration prevents virtually everyone from consciously perceiving the prime word [cf. @ForsterEtal2003, @Nievas2010], and thus presents a less contaminated estimate of early automatic processes in word recognition.\n\nParticipants were given 5 breaks throughout the experiment. When the experiment was over, the participants were then redirected to Prolific in order to validate their submission. The median time to finish the experiment was 11 minutes and 27 seconds. Each participant was paid with a standard rate of GBP 9/hour.\n\n## Data analysis {#sec-exp1-analysis}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# load the raw data dataframe\nexp1_data_folder <- \"experiment1\"\nexp1_rawdata_filename <- \"experiment_1_preprocessed_data.csv\"\n\n## 02. check if the rawdata file exists. if not, download it from OSF.\nif (!file.exists(here(exp1_data_folder, exp1_rawdata_filename))) {\n  osf_retrieve_file(\"ej8dh\") |> \n    osf_download(path = here(exp1_data_folder),\n                 conflicts = \"overwrite\") \n}\n\n## 03. read the data into R.\nexp1_rawdata <- here(exp1_data_folder, exp1_rawdata_filename) |>\n  read.csv(na = c(\"\", \"NA\")) %>%\n  filter(condition_rec != 'low') %>% # deleting the low-low frequency bin\n  mutate(condition_rec = fct_recode(condition_rec, low = \"mid\"), # renaming the mid-frequency bin as low\n         primeTime = primeDuration - maskDuration) # calculating the actual SOA\n\nexp1_info <- list()\nexp1_info$intended_prime_duration <- 33\nexp1_info$prime_dur_lb <- 25\nexp1_info$prime_dur_ub <- 60\nexp1_info$rt_lb <- 200\nexp1_info$rt_ub <- 1800\nexp1_info$freq_conditions <- c(\"high\", \"low\", \"non-word\")\nexp1_info$n_recruited <- exp1_rawdata$Rec_Session_Id |>\n  unique() |>\n  length()\n\nexp1_rawdata.sub <- exp1_rawdata %>%\n  filter(!is.na(TimeMeasure_Mean) & !is.na(primeDuration) & !is.na(responseError))\n            \n\nexp1_info$summary <- with(\n  transform(exp1_rawdata.sub,\n    RT_inrange = ifelse(RT >= exp1_info$rt_lb & RT <= exp1_info$rt_ub, 1, 0),\n    Prime_inrange = ifelse((primeDuration - maskDuration) >= exp1_info$prime_dur_lb &\n                             (primeDuration - maskDuration) <= exp1_info$prime_dur_ub, 1, 0),\n    list = Group_Nr),\n  {\n    data.frame(aggregate(Start_Time ~ Rec_Session_Id + Crowdsourcing_SubjId, data=exp1_rawdata.sub, unique),\n               aggregate(End_Time_Local ~ Rec_Session_Id + Crowdsourcing_SubjId, data=exp1_rawdata.sub, unique),\n              aggregate(cbind(list, SelectedGender, SelectedAge, TimeMeasure_Mean, TimeMeasure_Std) ~ Rec_Session_Id + Crowdsourcing_SubjId, data=exp1_rawdata.sub, unique),\n      aggregate(cbind(responseError, RT_inrange, Prime_inrange) ~ Rec_Session_Id + Crowdsourcing_SubjId, mean, data=exp1_rawdata.sub)\n  )\n}\n)\n\nexp1_info$summary <- exp1_info$summary[, -grep(\"Rec_Session_Id.|Crowdsourcing_SubjId.\", colnames(exp1_info$summary))] # remove all extra aggregating columns (subj ID)\n\nexp1_info$summary$Duration <- interval(ymd_hms(exp1_info$summary$Start_Time), \n                                             ymd_hms(exp1_info$summary$End_Time_Local)) |>\n                                      lapply(function(interval_value) {interval_value/dminutes(1)}) |> \n                                           unlist()\n```\n:::\n\n\n\n\nAnalysis scripts and an abridged version of the data collected can be found on online (<https://osf.io/ej8dh>). We performed three different steps of analyses (in sequential order), with the goal of only keeping data that pass a set of stringent including criteria (77,359 observations in total). After removing participants and items with high error rates, we implemented a novel pre-processing step looking at the distribution of the actual durations of prime stimuli of each trial and for each subject. This was necessary to understand the performance capabilities of experiments set up by *Labvanced*, and how accurate they are in keeping the prime duration constant. Finally, we removed RT outliers.\n\n### Step 1: subject and item performance {#sec-exp1-analysis-performance}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nexp1_step1_goodsubj <- exp1_info$summary |>\n  subset(responseError <= .3) \n\nexp1_step1_subj_remain <- exp1_step1_goodsubj |> nrow()\n\nexp1_step1_item.err <- exp1_rawdata.sub %>% group_by(condition_rec, target_rec) %>%\n  summarise(word.percent=mean(responseError)*100) %>% \n  filter(word.percent > 30)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'condition_rec'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nexp1_subj_filter_1 <- exp1_step1_goodsubj$Rec_Session_Id\nexp1_item_filter_1 <- exp1_step1_item.err$target_rec\n\nexp1_data_step1 <- exp1_rawdata.sub |>\n  subset(condition_rec %in% exp1_info$freq_conditions) |>\n  subset(Rec_Session_Id %in% exp1_subj_filter_1 & \n         !target_rec %in% exp1_item_filter_1)\n```\n:::\n\n\n\n\nItem and subject error rates were calculated, with a cutoff of 30%. Only 3 low-frequency words (*carte, parse, posit*), 5 non-words (**), and 8 participants were removed, with 291 participants remaining.\n\n### Step 2: prime durations {#sec-exp1-analysis-primeTime}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nexp1_summary.primeTime <- exp1_rawdata.sub %>% \n  summarise(meanPrimeTime = round(mean(primeTime), 2), \n            sdPrimeTime = round(sd(primeTime), 2))\n\nexp1_primeTimeRangeSummary <- exp1_rawdata.sub %>% \n  group_by(primeTime) %>%\n  mutate(range = ifelse(primeTime < exp1_info$prime_dur_lb, \"below\", \n                        ifelse(primeTime > exp1_info$prime_dur_ub, \"above\",\n                               \"in range\"))) %>% \n  group_by(range) %>% tally() %>% ungroup() %>%\n  mutate(range.percent = round((n*100)/nrow(exp1_rawdata.sub),2))\n\nexp1_data_step2 <- exp1_data_step1  |>\n  subset(primeTime >= exp1_info$prime_dur_lb & primeTime <= exp1_info$prime_dur_ub)\n\nexp1_step2_subj_remain <- exp1_data_step2$Rec_Session_Id |>\n  unique() |>\n  length()\n\nexp1_step2_trials_remain <- nrow(exp1_data_step2)\n```\n:::\n\n\n\n\nDuring the experiment, the duration of presentation of the prime word was recorded for every trial. Both the mean (mean = 37.88 ms) and the median (median = 35 ms)  of the actual prime durations were slightly larger than the intended prime duration (33 ms). This distribution suggests that, while overall the visual presentation was kept in most trials at the intended duration, it was not 100% as precise and accurate as dedicated presentation software installed on lab computers. This was expected and likely due to the inherent difficulty with timing precision of visual presentations in web browsers and the great variation of devices used by the participants. Both of these issues may be impossible to control, at least at the current state of browser development. However, in masked priming, in which the duration of the prime is essential part of the design itself, such fluctuations may indeed hinder proper elicitation of the priming response. As a way to counteract the potential influence that such fluctuations might have had on the priming response, we only kept trials whose prime durations were within a pre-set range from the intended prime duration of 33 ms. Taking a standard 60-Hz monitor as reference, the lower and the upper bounds were set respectively at 25 ms (i.e., the intended prime duration minus half of a full refresh cycle: $33-8~ ms$; noting that @Angele2023 already showed that no repetition priming effects are obtained with a 16.7ms prime duration) and 60 ms (i.e., the commonly accepted upper threshold of subliminal processing), so to remove as much as possible any trial that could have been consciously seen by participants. In the experiment tested, only 4% of the trials were out of the range selected. We take this as further corroborating evidence that *Labvanced* is pretty good at presenting stimuli at short durations, and the present, rather minimal fluctuations were due to external, and virtually incontrollable factors (such as CPU power, internet connection speed, and number of active operations in the background). The the out-of-range trial removal was performed on the data after the error rate removal procedure. A total of 291 participants and 67,209 observations were included in the next steps of analysis.\n\n### Step 3: RT distribution {#sec-exp1-analysis-RT}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# RT outliers \nexp1_data_step3 <- exp1_data_step2 |> \n  subset(RT >= exp1_info$rt_lb & RT <= exp1_info$rt_ub)\n\nexp1_step3_subj_remain <- exp1_data_step3$Rec_Session_Id |>\n  unique() |>\n  length()\n\nexp1_step3_trials_remain <- nrow(exp1_data_step3)\n\n# error trial removal\n\nexp1_data_step3b <- exp1_data_step3  |>\n  subset(responseError == 0)\n\nexp1_step3b_subj_remain <- exp1_data_step3b$Rec_Session_Id |>\n  unique() |>\n  length()\n\nexp1_step3b_trials_remain <- nrow(exp1_data_step3b)\n\n# remove subjects with less than 12 trials in at least one condition*primetype combination (half of the total number of items per combination)\nrt_data_labels <- c(\"Rec_Session_Id\", \"condition_rec\", \"primetype_rec\", \"RT\")\n\nexp1_subj_filter_2 <- exp1_data_step3b[, rt_data_labels] |>\n  aggregate(RT ~ ., FUN = length, drop = FALSE) |>\n  subset(RT < 12, select = Rec_Session_Id) |>\n  unique() |>\n  unlist()\n\nexp1_data_final <- exp1_data_step3b |>\n  subset(!(Rec_Session_Id %in% exp1_subj_filter_2))\n\nexp1_final_subj_remain <- exp1_data_final$Rec_Session_Id |>\n  unique() |> \n  length()\n  \nexp1_final_trials_remain <- nrow(exp1_data_final)\n```\n:::\n\n\n\n\nFinally, individual trials were excluded if their RT was below 200 ms and 1800 ms. 602 observations were excluded at this stage of analysis (i.e., 99.1% of the dataset). After removing incorrect trials, to ensure more accurate estimates, we also made sure that each condition (frequency*primetype) for each each participant ended up with at least half of the total number of trials presented (i.e., 12). A total of 61,449 observations and 282 subjects were included in the statistical analysis below.\n\n## Results {#sec-exp1-results}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# error rates averages\n### N.B.: to calculate error rates, will have to apply the 12-trial removal procedure after the RT removal procedure and without removing the error trials\nexp1_subj_filter_2_with.errors <- exp1_data_step3[, rt_data_labels] |> #exp1_data_step3 is the dataset after RT removal and before the error trial removal\n  aggregate(RT ~ ., FUN = length, drop = FALSE) |>\n  subset(RT < 12, select = Rec_Session_Id) |>\n  unique() |>\n  unlist()\n\nexp1_data_final_with.errors <- exp1_data_step3 |>\n  subset(!(Rec_Session_Id %in% exp1_subj_filter_2))\n\nexp1_error.rates <- exp1_data_final_with.errors %>%\n  mutate(primetype_rec = factor(primetype_rec, levels=c(\"unrelated\", \"related\")),\n         condition_rec = factor(condition_rec, levels=c(\"high\", \"low\", \"non-word\"))) %>%\n  group_by(condition_rec, primetype_rec, Rec_Session_Id) %>%\n  summarise(error.percent=mean(responseError)*100)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'condition_rec', 'primetype_rec'. You can\noverride using the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# RT averages\nexp1_rt.avg_subj <- exp1_data_final %>% \n  group_by(Rec_Session_Id, condition_rec, primetype_rec) %>%\n  summarise(meanRT = mean(RT))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'Rec_Session_Id', 'condition_rec'. You can\noverride using the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nexp1_rt_cor <- exp1_data_final %>% \n  group_by(Rec_Session_Id, condition_rec, primetype_rec) %>%\n  dplyr::summarise(meanRT=mean(RT)) %>%\n  select(Rec_Session_Id, condition_rec, primetype_rec, meanRT) %>% \n  pivot_wider(names_from='primetype_rec', values_from=c('meanRT')) %>%\n  group_by(condition_rec) %>%\n  summarise(cor=cor(unrelated, related))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'Rec_Session_Id', 'condition_rec'. You can\noverride using the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# RT + error averages (by subject)\nexp1_avgs_subj <- merge(exp1_rt.avg_subj, exp1_error.rates, by=c(\"Rec_Session_Id\", \"condition_rec\", \"primetype_rec\"))\n\nexp1_mop.err_across <- exp1_avgs_subj %>%\n  group_by(condition_rec, primetype_rec) %>%\n  summarise(gd.mean=mean(meanRT, na.rm=T), sd=sd(meanRT, na.rm=T), mean.error=mean(error.percent)) %>%\n  pivot_wider(id_cols=\"condition_rec\", names_from=primetype_rec, values_from=c(gd.mean, sd, mean.error)) %>%\n  left_join(exp1_rt_cor, by=\"condition_rec\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'condition_rec'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n#### ERROR PRIMING CALCULATIONS ####\n# we will just run stats for this, no numerical calculations (e.g., priming)\n\n### MAIN EFFECTS\n#### t-test\nexp1_errors_stats_main <- exp1_error.rates %>% \n  group_by(condition_rec) %>%\n  t_test(error.percent ~ primetype_rec, paired=T)\n\n#### RT PRIMING CALCULATIONS ####\n\n### MAIN EFFECTS\n#### by subject\nexp1_mop_subj <- exp1_rt.avg_subj %>% \n  pivot_wider(names_from=primetype_rec, values_from=meanRT) %>%\n  mutate(priming = unrelated-related)\n\n#### descriptive stats\nexp1_gdavg_mop <- exp1_mop_subj %>%\n  group_by(condition_rec) %>%\n  summarise(MOP = mean(priming), se = sd(priming)/sqrt(n()), ci=(qt(0.975, n()-1)*se),\n            sd=sd(priming), ES=round(MOP/sd, 2))\n\n#### summary table\nexp1_gdavg_mop_summary <- exp1_gdavg_mop %>%\n  left_join(., exp1_mop.err_across, by='condition_rec') %>%\n  mutate(across(c(2:5, 7:12), round), across(c(13), round, 2)) %>%\n  mutate(ci.lb = paste0(\"[\", MOP-ci), ci.ub = paste0(MOP+ci, \"]\")) %>%\n  unite(\"CI\", ci.lb:ci.ub, sep = \" \") %>% select(-ci, -se) %>% \n  rename(factor = \"condition_rec\") %>%\n  relocate(gd.mean_related:cor, .before=MOP) %>% \n  relocate(gd.mean_related, .after=gd.mean_unrelated) %>%\n  relocate(CI, .after=MOP)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: There was 1 warning in `mutate()`.\ni In argument: `across(c(13), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n#### t-test\nexp1_rt_stats_main <- exp1_rt.avg_subj %>%\n  mutate(primetype_rec = fct_relevel(primetype_rec, \"unrelated\", \"related\")) %>%\n  group_by(condition_rec) %>%\n  t_test(meanRT ~ primetype_rec, paired=T) %>% select(-2:-6) %>%\n  rename(factor = \"condition_rec\", t='statistic') %>%\n  mutate_if(is.numeric, list(~as.character(signif(., 3))))\n\n### INTERACTION EFFECT\n\n#### descriptive stats\nexp1_gdavg_fae <- exp1_mop_subj %>%\n  select(-unrelated, -related) %>%\n  pivot_wider(names_from=\"condition_rec\", values_from=\"priming\") %>%\n  mutate(interaction = low-high) %>% ungroup() %>%\n  summarise(factor = \"frequency:primetype\", \n            sd=sd(interaction), se = sd/sqrt(n()), ci = (qt(0.975, n()-1)*se)) %>%\n  mutate(across(c(2:4), round))\n\n##### summary table\nexp1_gdavg_fae_summary <- exp1_mop_subj %>% \n  select(-unrelated, -related) %>% ungroup() %>%\n  pivot_wider(names_from=\"condition_rec\", values_from=\"priming\") %>%\n  summarise(mean_high = round(mean(high)), mean_low = round(mean(low)), cor=round(cor(high, low), 3)) %>%\n  bind_cols(., exp1_gdavg_fae) %>%\n  mutate(MOP = mean_low-mean_high, ES = round(MOP/sd, 2), ci.lb = paste0(\"[\", MOP-ci), ci.ub = paste0(MOP+ci, \"]\")) %>%\n  unite(\"CI\", ci.lb:ci.ub, sep = \" \") %>% select(-ci, -se) %>% \n  relocate(CI, .after=MOP) %>%\n  relocate(factor, .before=mean_high)\n  \n#### t-test\nexp1_rt_stats_interaction <- exp1_mop_subj %>% ungroup() %>%\n  filter(condition_rec != 'non-word') %>%\n  mutate(condition_rec = fct_relevel(condition_rec, \"low\", \"high\")) %>%\n  t_test(priming ~ condition_rec, paired=TRUE) %>% \n  mutate_if(is.numeric, list(~as.character(signif(., 3)))) %>%\n  select(-1, -4:-5) %>%\n  unite(\"factor\", group1:group2) %>%\n  rename(t='statistic') %>%\n  mutate(factor = \"frequency:primetype\")\n```\n:::\n\n\n\n\nFor each frequency bin, priming effects were calculated for each subject by subtracting the subject's mean RT to the related condition from the subject's mean RT to the unrelated condition. Unstandardized (in ms) and standardized effect sizes (i.e., Cohen's *d*) were then calculated for each condition. @tbl-statsResults below reports the descriptive and inferential statistics of the experiment. Both frequency conditions show statistically significant repetition priming effects (*MOP_HF* = 23, CI_95% = [19, 27], *t*(281) = 10.4, $p<.0001$; *MOP_LF* = 30, CI_95% = [24, 36], *t*(281) = 9.75, $p<.0001$), whereas the non-word priming effects were right at the alpha-level (*MOP_* = -4, CI_95% = [-8, 0], *t*(281) = -1.91, $p=0.057$). The low-frequency repetition priming effect was 7-ms larger than that of the high-frequency words, but this FAE effect was only marginally significant (*M_FAE* = 7, CI_95% = [-1, 15]), *t*(281) = 1.88, $p=0.06$). As for the error analysis, we found a significant priming effect in all conditions (high: *t*(281)=2.51, $p<.0001$; low: *t*(281)=6.39, $p<.0001$; non-word: *t*(281)=-2.24, $p<.0001$). \n\n\\blandscape\n\n\n\n::: {#tbl-statsResults .cell tbl-cap='Experiment 1. Summary of the word priming results. *Legend.* MOP: magnitude of priming.' tbl-pos='h'}\n\n```{.r .cell-code .hidden}\nexp1_summary.results_mop <- merge(exp1_gdavg_mop_summary, exp1_rt_stats_main, by='factor')\nexp1_summary.results_fae <- merge(exp1_gdavg_fae_summary, exp1_rt_stats_interaction, by='factor') |>\n  select(-mean_high, -mean_low)\n\nexp1_summary.results <- bind_rows(exp1_summary.results_mop, exp1_summary.results_fae)\n  \nexp1_summary.results %>%\n  relocate(c(\"sd_unrelated\", \"mean.error_unrelated\"), .before=gd.mean_related) %>%\n  gt() %>%\n  cols_label(\n    CI = \"95% CI\",\n    contains(\"mean\") ~ \"mean\",\n    contains(\"sd\") ~ \"SD\", \n    contains(\"error\") ~ \"Error (%)\"\n  ) %>%\n  tab_spanner(\n    label = \"unrelated RT\",\n    columns = c(2:4)\n  ) %>%\n  tab_spanner(\n    label = \"repetition RT\",\n    columns = c(5:7)\n  ) %>%\n  tab_spanner(\n    label = 'priming effects',\n    columns = c(9:12)\n  ) %>%\n  tab_spanner(\n    label = md(\"_t_-test\"),\n    columns = c(13:15)\n  ) %>%\n  cols_label(\n    sd = md(\"SD~p~\")\n  ) %>%\n  cols_label(\n    t = md(\"_t_\"),\n    p = md(\"_p_\"),\n  ) %>%\n   sub_missing(\n    missing_text = \" \"\n  )\n```\n\n::: {.cell-output-display}\n\\begin{longtable}{lrrrrrrrrlrrrrl}\n\\toprule\n & \\multicolumn{3}{c}{unrelated RT} & \\multicolumn{3}{c}{repetition RT} &  & \\multicolumn{4}{c}{priming effects} & \\multicolumn{3}{c}{\\emph{t}-test} \\\\ \n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){9-12} \\cmidrule(lr){13-15}\nfactor & mean & SD & Error (\\%) & mean & SD & Error (\\%) & cor & MOP & 95\\% CI & SD\\textsubscript{p} & ES & \\emph{t} & df & \\emph{p} \\\\ \n\\midrule\\addlinespace[2.5pt]\nhigh & 619 & 77 & 2 & 596 & 80 & 1 & 0.89 & 23 & [19 27] & 37 & 0.62 & 10.4 & 281 & 8.78e-22 \\\\ \nlow & 699 & 93 & 10 & 669 & 91 & 7 & 0.84 & 30 & [24 36] & 52 & 0.58 & 9.75 & 281 & 1.51e-19 \\\\ \nnon-word & 712 & 110 & 6 & 716 & 110 & 6 & 0.96 & -4 & [-8 0] & 31 & -0.11 & -1.91 & 281 & 0.0567 \\\\ \nfrequency:primetype &   &   &   &   &   &   & -0.01 & 7 & [-1 15] & 64 & 0.11 & 1.88 & 281 & 0.0616 \\\\ \n\\bottomrule\n\\end{longtable}\n\n:::\n:::\n\n\n\n\\elandscape \n\n## Discussion {#sec-exp1-discussion}\n\nThe primary objective of Experiment 1 was to present findings from a typical masked repetition priming experiment conducted online and to evaluate whether contemporary online stimulus delivery programs, such as *Labvanced*, can yield data comparable in quality to traditional lab-based experiments. The results indicate that online experiments using *Labvanced* can indeed provide masked priming data of satisfactory quality with some precautionary considerations in data analysis.\n\nFirst, the error rate was found significant in all priming conditions, with the non-word condition triggering inhibitory effects, in line with the previous literature [@ForsterDavis1984, experiment 1 and 2\\; @Forster1999]. More crucially for the question being asked here, we found statistically significant masked priming effects in the response to both high- and low-frequency conditions, and a marginally significant FAE effect (with an effect size of 7 ms). As noted elsewhere [@PotvinSchtuz2000], the absence of a significant interaction effect may easily arise due to low statistical power. To address this concern, Experiment 2 employed a sample size determined by the power analysis simulations mentioned above, ensuring acceptable statistical power ($1-\\beta>80%$) to detect the potential interaction between priming and frequency.\n\n# Experiment 2\n\nThe findings from Experiment 1, as well as those reported by @Angele2023 and @Cayado2023, establish the feasibility of obtaining masked repetition priming in online experiments with a 33 ms prime duration. However, a crucial question remains: can we reliably detect the Frequency Attenuation Effect (FAE) under these online settings? Experiment 2 directly addresses concerns about potential statistical power limitations observed in Experiment 1 and much of the prior literature. Specifically targeting what we construe as the smallest theoretically interesting FAE (5ms), we recruited a larger sample size, as determined by a power analysis simulation. We simulated 10,000 datasets for each of the combinations of two statistical parameters (standard deviation, correlation conditions, which were kept uniform for simplicity) for various sample sizes and hypothesized FAEs. Based on our own pilot studies and previous published work [@Petrosino2020; @PetrosinoEtal2023], the simulations involved the standard deviation ranging between 80 and 120 ms (with 10 ms increments), the correlation between 0.7 and 0.9 (with 0.1 increments), and the sample size between 200 and 3,000 participants (with 150 unit increments). Three different FAE sizes were chosen: 15 ms, 10 ms and 5 ms. The first effect size (15 ms) is about half of the ones observed in the studies that had a significant interaction (~30 ms). The second effect size (10 ms) is close to the size of the average frequency attenuation effect found in the literature (13 ms). The last effect size (5 ms) is our lower-bound estimate of a theoretically interesting effect size. The code used for the power simulations, along with the simulated datasets are available online (<https://osf.io/r7d2q/>).\n\nOur analysis identified a sample size of 1,250 participants as optimal, ensuring robust statistical power (> 80%) across various parameter combinations (@fig-power-1250), especially for raw FAEs equal to or exceeding 10 ms â€”- a value closely aligned with the average FAE calculated from previous studies (refer to @tbl-litReview). In light of the observed limitations in the temporal accuracy and precision of current online stimulus delivery programs (discussed in @sec-exp1-analysis-primeTime), which necessitated substantial subject and data exclusion in Experiment 1, we aimed for an intended sample size of 2,600. This decision was made to enhance the likelihood of obtaining our target sample size of 1,250 participants after applying all the necessary exclusion criteria to the data.\n\n\n\n\n::: {#cell-fig-power-1250 .cell}\n\n```{.r .cell-code .hidden}\nload(\"/Users/rp3650/Library/CloudStorage/GoogleDrive-robpetrosino@gmail.com/My Drive/Academics/projects/morphology/morphological-decomposition/sub-projects/frequency-effects/frequency-effects_power-analysis/freq_atten_exp2.5ms.RData\")\n\nload(\"/Users/rp3650/Library/CloudStorage/GoogleDrive-robpetrosino@gmail.com/My Drive/Academics/projects/morphology/morphological-decomposition/sub-projects/frequency-effects/frequency-effects_power-analysis/freq_atten_exp2.10ms.RData\")\n\nload(\"/Users/rp3650/Library/CloudStorage/GoogleDrive-robpetrosino@gmail.com/My Drive/Academics/projects/morphology/morphological-decomposition/sub-projects/frequency-effects/frequency-effects_power-analysis/freq_atten_exp2.15ms.RData\")\n\nfreq_atten_exp2.5ms_df.sub <- freq_atten_exp2.5ms_df %>% \n  filter(ES == 5) %>%\n  filter(std_dev >= 80 & std_dev <= 120) %>%\n  filter(rho >= 0.7 & rho <= 0.9)\n\nfreq_atten_exp2.10ms_df.sub <- freq_atten_exp2.10ms_df %>% \n  filter(ES == 10) %>%\n  filter(std_dev >= 80 & std_dev <= 120) %>%\n  filter(rho >= 0.7 & rho <= 0.9)\n\nfreq_atten_exp2.15ms_df.sub <- freq_atten_exp2.15ms_df %>% \n  filter(ES == 15 & grepl(\"interaction\", row.names(freq_atten_exp2.15ms_df))) %>%\n  filter(std_dev >= 80 & std_dev <= 120) %>%\n  filter(rho >= 0.7 & rho <= 0.9)\n\npower_pred_df <- bind_rows(freq_atten_exp2.5ms_df.sub, freq_atten_exp2.10ms_df.sub, freq_atten_exp2.15ms_df.sub) \n\npower_pred_df.sub1250 <- power_pred_df %>%\n  filter(nsubj == 1700)\n\npower_pred_df.sub1250 |>\n  ggplot(aes(y = power_unadjusted, x = ES)) + \n    geom_line() + \n    geom_point() +\n    geom_hline(yintercept = 0.8, color = \"red2\") + \n    facet_grid(rho ~ std_dev) +\n  scale_x_continuous(breaks=seq(5, 15, 5), limits=c(5, 15), labels=seq(5, 15, 5),\n    sec.axis = sec_axis(~ . , name = \"sd\", breaks = NULL, labels = NULL)) +\n  scale_y_continuous(breaks=seq(0, 1, 0.2), limits=c(0, 1), labels=seq(0, 1, .2),\n    sec.axis = sec_axis(~ . , name = \"cor\", breaks = NULL, labels = NULL)) +\n  theme_bw()+\n    theme(#axis.line = element_line(colour = \"black\"),\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(), \n      panel.border = element_blank(),\n      legend.position=\"none\"\n      #panel.background = element_blank() \n     )+\n  labs(title=\"N=1,250\", y = \"power (unadjusted)\", x = \"FAE size (ms)\")\n```\n\n::: {.cell-output-display}\n![Power simulations with a sample size of 1,250, for all combinations of standard deviation (sd), pairwise correlation (cor), and interaction effect size. The red line identifies the threshold of 80% power.](index_files/figure-pdf/fig-power-1250-1.pdf){#fig-power-1250 fig-pos='H'}\n:::\n:::\n\n\n\n\n## Methods {#sec-exp2-methods}\n\n### Preregistration {#sec-exp2-prereg}\n\nWe preregistered the results of the power analysis, the goals, and the design and analysis plan for experiment 2 prior to data collection. The preregistration, containing the desired sample size, included variables, hypotheses, and planned analyses is available on online (<https://doi.org/10.17605/OSF.IO/3NFQP>).\n\n### Materials {#sec-exp2-methods-materials}\n\nOne-hundred and four five-letter words, half of low frequency (between 7 and 24 in the SUBTLEX$_{US}$ frequency per million) and half of high frequency (between 57 and 2,961 in the SUBTLEX$_{US}$ frequency per million) were sampled from ELP [@balota2007], but this time based on the SUBTLEX$_{US}$ frequency counts rather than HAL as experiment 1. @tbl-words_exp2 shows that although the SUBTLEX$_{US}$ frequency ranges of the two conditions were very far from one another (similarly to what was done in Experiment 1; @sec-exp1-methods-materials), they still show some overlap in when HAL frequencies are used. As mentioned before, this seems to be a general problem when considering different frequency databases at the same time for a smaller set of stimuli that need to be manipulated and controlled in different ways (see also fn. \\ref{fn-databases}). From each word set, fifty words were randomly chosen to be presented as targets and related primes (the *related* condition), and the remaining fifty were presented as unrelated primes (the *unrelated* condition). All words used were monomorphemic nouns, adjectives, or verbs, thus excluding particles, prepositions, and derived or inflected forms.\n\n\n\n\n::: {#tbl-words_exp2 .cell tbl-cap='Experiment 2. Descriptive statistics of the word items used. For both frequency databases, the word frequencies were converted to per-million count to ensure cross-comparison.'}\n\n```{.r .cell-code .hidden}\nwords_exp2 <- read.csv('./materials/experiment2/frequency-effects_experiment2_stimuli.csv') %>% filter(type=='word')\n\nneeds_mutated <- function(x) {\n  if(!is.numeric(x)) return(FALSE)\n  max(x, na.rm = TRUE) > 100\n}\n\nwords_exp2 %>%\n  group_by(freq.bin) %>%\n  mutate(Freq_HAL.Pm = (Freq_HAL * 10^6)/(131 * 10^6)) %>%\n  summarise(N = n(),\n            minFreq= min(Freq_HAL.Pm, na.rm = T), maxFreq=max(Freq_HAL.Pm, na.rm = T),\n            meanFreq = mean(Freq_HAL.Pm, na.rm = T), sdFreq = sd(Freq_HAL.Pm, na.rm = T),\n            minSUBFreq= min(SUBTLWF, na.rm = T), maxSUBFreq=max(SUBTLWF, na.rm = T),\n            meanSUBFreq = mean(SUBTLWF, na.rm = T), sdSUBFreq = sd(SUBTLWF, na.rm = T),\n            #meanRT=mean(as.numeric(I_Mean_RT), na.rm = T), sdRT=sd(as.numeric(I_Mean_RT), na.rm = T),\n            #meanLength = mean(Length, na.rm = T), sdLength=sd(Length, na.rm = T)\n            ) %>% \n  mutate_if(is.numeric, ~round(.)) %>% \n  mutate_if(needs_mutated, ~round(.)) %>%\n  gt() %>%\n  cols_label(\n    freq.bin = \"frequency\", \n    minFreq = \"min\", maxFreq = \"max\", meanFreq = \"mean\", sdFreq = \"SD\", \n    minSUBFreq = \"min\", maxSUBFreq = \"max\", meanSUBFreq = \"mean\", sdSUBFreq = \"SD\") %>%\n  tab_spanner(\n    label=md(\"**HAL**\"),\n    columns=3:6\n  ) %>%\n  tab_spanner(\n    label=md(\"**SUBTLEX~US~**\"),\n    columns=7:10\n  )\n```\n\n::: {.cell-output-display}\n\\begin{longtable}{lrrrrrrrrr}\n\\toprule\n &  & \\multicolumn{4}{c}{\\textbf{HAL}} & \\multicolumn{4}{c}{\\textbf{SUBTLEX\\textsubscript{US}}} \\\\ \n\\cmidrule(lr){3-6} \\cmidrule(lr){7-10}\nfrequency & N & min & max & mean & SD & min & max & mean & SD \\\\ \n\\midrule\\addlinespace[2.5pt]\nhigh & 52 & 45 & 4984 & 573 & 808 & 57 & 2691 & 210 & 388 \\\\ \nlow & 52 & 6 & 570 & 64 & 93 & 7 & 24 & 13 & 5 \\\\ \n\\bottomrule\n\\end{longtable}\n\n:::\n:::\n\n\n\n\nOne-hundred and four five-letter, phonotactically legal nonwords were randomly selected from the ELP database as well. Half of them were randomly selected to be presented as targets; the other half was instead used as unrelated nonword primes. None of the nonwords contained any existing English morpheme. Both the words and non-words used in the experiments are reported in the appendix below.\n\n### Participants {#sec-exp2-methods-participants}\n\nTwo thousand and six hundred participants (1445 females; age mean: 42.31; age sd: 14.12) were recruited on Prolific (<https://www.prolific.com>) with the same criteria specified for experiment 1 (@sec-exp1-methods-participants).\n\n### Procedure {#sec-exp2-methods-proc}\n\nExperiment 2 was conducted in the same way as experiment 1 (see @sec-exp1-methods-proc). The median time to finish the experiment was around 5 minutes.\n\n## Data analysis {#sec-exp2-analysis}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# load the raw data dataframe\nexp2_data_folder <- \"experiment2\"\nexp2_rawdata_filename <- \"experiment_2_preprocessed_data.csv\"\n\n## 02. check if the rawdata file exists. if not, download it from OSF.\nif (!file.exists(here(exp2_data_folder, exp2_rawdata_filename))) {\n  osf_retrieve_file(\"vn3r2\") |> \n    osf_download(path = here(exp2_data_folder),\n                 conflicts = \"overwrite\") \n}\n\n## 03. read the data into R.\nexp2_rawdata <- here(exp2_data_folder, exp2_rawdata_filename) |>\n  read.csv(na = c(\"\", \"NA\")) %>%\n  mutate(primeTime = primeDuration - maskDuration) # calculating the actual SOA\n\nexp2_info <- list()\nexp2_info$intended_prime_duration <- 33\nexp2_info$prime_dur_lb <- 25\nexp2_info$prime_dur_ub <- 60\nexp2_info$rt_lb <- 200\nexp2_info$rt_ub <- 1800\nexp2_info$freq_conditions <- c(\"high\", \"low\", \"non-word\")\nexp2_info$n_recruited <- exp2_rawdata$Rec_Session_Id |>\n  unique() |>\n  length()\n\nexp2_rawdata.sub <- exp2_rawdata %>%\n  filter(!is.na(TimeMeasure_Mean) & !is.na(primeDuration) & !is.na(responseError))\n            \nexp2_info$summary <- with(\n  transform(exp2_rawdata.sub,\n    RT_inrange = ifelse(RT >= exp2_info$rt_lb & RT <= exp2_info$rt_ub, 1, 0),\n    Prime_inrange = ifelse((primeDuration - maskDuration) >= exp2_info$prime_dur_lb &\n                             (primeDuration - maskDuration) <= exp2_info$prime_dur_ub, 1, 0)),\n  {\n    data.frame(aggregate(Start_Time ~ Rec_Session_Id + Crowdsourcing_SubjId, data=exp2_rawdata.sub, unique),\n               aggregate(End_Time_Local ~ Rec_Session_Id + Crowdsourcing_SubjId, data=exp2_rawdata.sub, unique),\n              aggregate(cbind(list, SelectedGender, SelectedAge, TimeMeasure_Mean, TimeMeasure_Std) ~ Rec_Session_Id + Crowdsourcing_SubjId, data=exp2_rawdata.sub, unique),\n      aggregate(cbind(responseError, RT_inrange, Prime_inrange) ~ Rec_Session_Id + Crowdsourcing_SubjId, mean, data=exp2_rawdata.sub)\n  )\n}\n)\n\nexp2_info$summary <- exp2_info$summary[, -grep(\"Rec_Session_Id.|Crowdsourcing_SubjId.\", colnames(exp2_info$summary))] # remove all extra aggregating columns (subj ID)\n\nexp2_info$summary$Duration <- interval(ymd_hms(exp2_info$summary$Start_Time), \n                                             ymd_hms(exp2_info$summary$End_Time_Local)) |>\n                                      lapply(function(interval_value) {interval_value/dminutes(1)}) |> \n                                           unlist()\n```\n:::\n\n\n\n\nAnalysis scripts and an abridged version of the data collected can be found online (<https://osf.io/vn3r2>), and consisted of 297,598 observations in total. We performed the same three steps of analysis described for experiment 1 (@sec-exp1-analysis).\n\n### Step 1: subject and item performance {#sec-exp2-analysis-performance}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nexp2_step1_goodsubj <- exp2_info$summary |>\n  subset(responseError <= .3) \n\nexp2_step1_subj_remain <- exp2_step1_goodsubj |> nrow()\n\nexp2_step1_item.err <- exp2_rawdata.sub %>% group_by(condition_rec, target_rec) %>%\n  summarise(word.percent=mean(responseError)*100) %>% \n  filter(word.percent > 30)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'condition_rec'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nexp2_subj_filter_1 <- exp2_step1_goodsubj$Rec_Session_Id\nexp2_item_filter_1 <- exp2_step1_item.err$target_rec\n\nexp2_data_step1 <- exp2_rawdata.sub |>\n  subset(condition_rec %in% exp2_info$freq_conditions) |>\n  subset(Rec_Session_Id %in% exp2_subj_filter_1 & \n         !target_rec %in% exp2_item_filter_1)\n```\n:::\n\n\n\n\nSimilarly to experiment 1, item and subject error rates were calculated. The item error rate was never below above 14%, so no item was excluded from analysis. 19 subjects were removed because their error rate was above 30%. Thus, a total of 269,652 observations and 2,593 participants were included in further analyses.\n\n### Step 2: prime durations {#sec-exp2-analysis-primeTime}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nexp2_summary.primeTime <- exp2_rawdata.sub %>% \n  summarise(meanPrimeTime = round(mean(primeTime), 2), \n            sdPrimeTime = round(sd(primeTime), 2))\n\nexp2_primeTimeRangeSummary <- exp2_rawdata.sub %>% \n  group_by(primeTime) %>%\n  mutate(range = ifelse(primeTime < exp2_info$prime_dur_lb, \"below\", \n                        ifelse(primeTime > exp2_info$prime_dur_ub, \"above\",\n                               \"in range\"))) %>% \n  group_by(range) %>% tally() %>% ungroup() %>%\n  mutate(range.percent = round((n*100)/nrow(exp2_rawdata.sub),2))\n\nexp2_data_step2 <- exp2_data_step1  |>\n  subset(primeTime >= exp2_info$prime_dur_lb & primeTime <= exp2_info$prime_dur_ub)\n\nexp2_step2_subj_remain <- exp2_data_step2$Rec_Session_Id |>\n  unique() |>\n  length()\n\nexp2_step2_trials_remain <- nrow(exp2_data_step2)\n```\n:::\n\n\n\n\nPrime fluctuations were dealt with in the same way as in experiment 1 (@sec-exp1-analysis-primeTime). As compared to experiment 1, this time the mean (mean = 32.32 ms, sd = 15) and the median (median = 33 ms)  were closer to the intended prime duration (33 ms). The prime duration cut-off set for experiment 1 (i.e., any trial whose prime duration was out of the 25-60ms range) removed 13 % of the trials. No participant was excluded, for a total of 237,287 observations.\n\n### Step 3: RT distribution {#sec-exp2-analysis-RT}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# RT outliers \nexp2_data_step3 <- exp2_data_step2 |> \n  subset(RT >= exp2_info$rt_lb & RT <= exp2_info$rt_ub)\n\nexp2_step3_subj_remain <- exp2_data_step3$Rec_Session_Id |>\n  unique() |>\n  length()\n\nexp2_step3_trials_remain <- nrow(exp2_data_step3)\n\n# error trial removal\n\nexp2_data_step3b <- exp2_data_step3  |>\n  subset(responseError == 0)\n\nexp2_step3b_subj_remain <- exp2_data_step3b$Rec_Session_Id |>\n  unique() |>\n  length()\n\nexp2_step3b_trials_remain <- nrow(exp2_data_step3b)\n\n# remove subjects with less than 12 trials in at least one condition*primetype combination (half of the total number of items per combination)\nrt_data_labels <- c(\"Rec_Session_Id\", \"condition_rec\", \"primetype_rec\", \"RT\")\n\nexp2_subj_filter_2 <- exp2_data_step3b[, rt_data_labels] |>\n  aggregate(RT ~ ., FUN = length, drop = FALSE) |>\n  subset(RT < 7, select = Rec_Session_Id) |>\n  unique() |>\n  unlist()\n\n### we also want to sure that all subjects have all conditions; in case some subject had all the trials for a given condition lost down the road, they will be removed\nexp2_subj_filter_conditions <- \n  exp2_data_step3b %>%\n  group_by(Rec_Session_Id) %>% \n  distinct(condition_rec, primetype_rec) %>% \n  tally() %>% filter(n != 6) %>% pull(Rec_Session_Id)\n\nexp2_data_final <- exp2_data_step3b |>\n  subset(!(Rec_Session_Id %in% exp2_subj_filter_2) & !(Rec_Session_Id %in% exp2_subj_filter_conditions))\n\nexp2_final_subj_remain <- exp2_data_final$Rec_Session_Id |>\n  unique() |> \n  length()\n  \nexp2_final_trials_remain <- nrow(exp2_data_final)\n```\n:::\n\n\n\n\nAfter removing the incorrect responses, similarly to what we did for experiment 1 (@sec-exp1-analysis-RT), 0.51% of the trials were excluded if the relative RT was below 200 ms and above 1800 ms, Finally, 249 subjects were removed because the number of trials within the same condition was less than 7 (i.e., about half of the total number of trials being presented within the same condition, i.e. 13). A total of 210,889 observations and 2,341 subjects were included in the statistical analysis below. \n\n## Results {#sec-exp2-results}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# error rates averages\n### we also want to sure that all subjects have all conditions; in case some subject had all the trials for a given condition lost down the road, they will be removed. Crucially the trial calculations are made on the dataset *before* the trial error removal step\nexp2_subj_filter_2_with.errors <- exp2_data_step3[, rt_data_labels] |>\n  aggregate(RT ~ ., FUN = length, drop = FALSE) |>\n  subset(RT < 7, select = Rec_Session_Id) |>\n  unique() |>\n  unlist()\n# this step just makes sure that the same subjects will be removed from both datasets\nexp2_subj_filter_2_with.errors <- union(exp2_subj_filter_2_with.errors, exp2_subj_filter_2)\n\n### just making sure that all subjects have all conditions; in case some subject had all the trials for a given condition lost down the road, they will be removed\nexp2_subj_filter_conditions_with.errors <- \n  exp2_data_step3 %>%\n  group_by(Rec_Session_Id) %>% \n  distinct(condition_rec, primetype_rec) %>% \n  tally() %>% filter(n != 6) %>% pull(Rec_Session_Id)\n\nexp2_data_final_with.errors <- exp2_data_step3 |>\n  subset(!(Rec_Session_Id %in% exp2_subj_filter_2_with.errors) & \n           !(Rec_Session_Id %in% exp2_subj_filter_conditions_with.errors)) \n\nexp2_error.rates <- exp2_data_final_with.errors %>%\n  mutate(primetype_rec = factor(primetype_rec, levels=c(\"unrelated\", \"related\")),\n         condition_rec = factor(condition_rec, levels=c(\"high\", \"low\", \"non-word\"))) %>%\n  group_by(condition_rec, primetype_rec, Rec_Session_Id) %>%\n  summarise(error.percent=mean(responseError)*100)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'condition_rec', 'primetype_rec'. You can\noverride using the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# RT averages\nexp2_rt.avg_subj <- exp2_data_final %>% \n  group_by(Rec_Session_Id, condition_rec, primetype_rec) %>%\n  summarise(meanRT = mean(RT))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'Rec_Session_Id', 'condition_rec'. You can\noverride using the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nexp2_rt_cor <- exp2_data_final %>% \n  group_by(Rec_Session_Id, condition_rec, primetype_rec) %>%\n  dplyr::summarise(meanRT=mean(RT)) %>%\n  select(Rec_Session_Id, condition_rec, primetype_rec, meanRT) %>% \n  pivot_wider(names_from='primetype_rec', values_from=c('meanRT')) %>%\n  group_by(condition_rec) %>%\n  summarise(cor=cor(unrelated, related))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'Rec_Session_Id', 'condition_rec'. You can\noverride using the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# RT + error averages (by subject)\nexp2_avgs_subj <- merge(exp2_rt.avg_subj, exp2_error.rates, by=c(\"Rec_Session_Id\", \"condition_rec\", \"primetype_rec\"))\n\nexp2_mop.err_across <- exp2_avgs_subj %>%\n  group_by(condition_rec, primetype_rec) %>%\n  summarise(gd.mean=mean(meanRT, na.rm=T), sd=sd(meanRT, na.rm=T), mean.error=mean(error.percent)) %>%\n  pivot_wider(id_cols=\"condition_rec\", names_from=primetype_rec, values_from=c(gd.mean, sd, mean.error)) %>%\n  left_join(exp2_rt_cor, by=\"condition_rec\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'condition_rec'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n#### ERROR PRIMING CALCULATIONS ####\n# we will just run stats for this, no numerical calculations (e.g., priming)\n\n### MAIN EFFECTS\n#### t-test\nexp2_errors_stats_main <- exp2_error.rates %>% \n  group_by(condition_rec) %>%\n  t_test(error.percent ~ primetype_rec, paired=T)\n\n#### RT PRIMING CALCULATIONS ####\n\n### MAIN EFFECTS\n#### by subject\nexp2_mop_subj <- exp2_rt.avg_subj %>% \n  pivot_wider(names_from=primetype_rec, values_from=meanRT) %>%\n  mutate(priming = unrelated-related)\n\n#### descriptive stats\nexp2_gdavg_mop <- exp2_mop_subj %>%\n  group_by(condition_rec) %>%\n  summarise(MOP = mean(priming), se = sd(priming)/sqrt(n()), ci=(qt(0.975, n()-1)*se),\n            sd=sd(priming), ES=round(MOP/sd, 2))\n\n#### summary table\nexp2_gdavg_mop_summary <- exp2_gdavg_mop %>%\n  left_join(., exp2_mop.err_across, by='condition_rec') %>%\n  mutate(across(c(2:5, 7:12), round), across(c(13), round, 2)) %>%\n  mutate(ci.lb = paste0(\"[\", MOP-ci), ci.ub = paste0(MOP+ci, \"]\")) %>%\n  unite(\"CI\", ci.lb:ci.ub, sep = \" \") %>% select(-ci, -se) %>% \n  rename(factor = \"condition_rec\") %>%\n  relocate(gd.mean_related:cor, .before=MOP) %>% \n  relocate(gd.mean_related, .after=gd.mean_unrelated) %>%\n  relocate(CI, .after=MOP)\n\n#### t-test\nexp2_rt_stats_main <- exp2_rt.avg_subj %>%\n  mutate(primetype_rec = fct_relevel(primetype_rec, \"unrelated\", \"related\")) %>%\n  group_by(condition_rec) %>%\n  t_test(meanRT ~ primetype_rec, paired=T) %>% select(-2:-6) %>%\n  rename(factor = \"condition_rec\", t='statistic') %>%\n  mutate_if(is.numeric, list(~as.character(signif(., 3))))\n\n### INTERACTION EFFECT\n\n#### descriptive stats\nexp2_gdavg_fae <- exp2_mop_subj %>%\n  select(-unrelated, -related) %>%\n  pivot_wider(names_from=\"condition_rec\", values_from=\"priming\") %>%\n  mutate(interaction = low-high) %>% ungroup() %>%\n  summarise(factor = \"frequency:primetype\", \n            sd=sd(interaction), se = sd/sqrt(n()), ci = (qt(0.975, n()-1)*se)) %>%\n  mutate(across(c(2:4), round))\n\n##### summary table\nexp2_gdavg_fae_summary <- exp2_mop_subj %>% \n  select(-unrelated, -related) %>% ungroup() %>%\n  pivot_wider(names_from=\"condition_rec\", values_from=\"priming\") %>%\n  summarise(mean_high = round(mean(high)), mean_low = round(mean(low)), cor=round(cor(high, low), 3)) %>%\n  bind_cols(., exp2_gdavg_fae) %>%\n  mutate(MOP = mean_low-mean_high, ES = round(MOP/sd, 2), ci.lb = paste0(\"[\", MOP-ci), ci.ub = paste0(MOP+ci, \"]\")) %>%\n  unite(\"CI\", ci.lb:ci.ub, sep = \" \") %>% select(-ci, -se) %>% \n  relocate(CI, .after=MOP) %>%\n  relocate(factor, .before=mean_high)\n  \n#### t-test\nexp2_rt_stats_interaction <- exp2_mop_subj %>% ungroup() %>%\n  filter(condition_rec != 'non-word') %>%\n  mutate(condition_rec = fct_relevel(condition_rec, \"low\", \"high\")) %>%\n  t_test(priming ~ condition_rec, paired=TRUE) %>% \n  mutate_if(is.numeric, list(~as.character(signif(., 3)))) %>%\n  select(-1, -4:-5) %>%\n  unite(\"factor\", group1:group2) %>%\n  rename(t='statistic') %>%\n  mutate(factor = \"frequency:primetype\")\n```\n:::\n\n\n\n\nFor each frequency condition, priming effects were calculated in the same way as experiment 1. @tbl-exp2-statsResults below report the descriptive statistics of the experiment. All three conditions showed statistically significant repetition priming effects (*MOP_HF* = 18, CI_95% = [16 20], t(2340) = 19.7, $p<.0001$; *MOP_LF* = 28, CI_95% = [26 30], *t*(2340) = 27.8, $p<.0001$; *MOP_NW* = -2, CI_95% = [-4 0], t(2340) = -2.33, $p<.0001$). The low-frequency word repetition priming effect was 10 ms larger than the high-frequency word repetition priming effect, and this FAE effect was statistically significant (*M_FAE* = 10, CI_95% = [7 13]), *t*(2340) = 7.24, $p<.0001$). As for the word error analysis, we found significant priming effects in the word conditions (high: *t*(2340)=9.95, *p*<.0001; low: *t*(2340)=16.9, *p*<.0001), as well as in the non-word condition (non-word: *t*(2340)=-3.27, *p*=.001). \n\n\\blandscape\n\n\n\n::: {#tbl-exp2-statsResults .cell tbl-cap='Experiment 2. Summary of the word priming results. *Legend.* MOP: magnitude of priming.' tbl-pos='h'}\n\n```{.r .cell-code .hidden}\nexp2_summary.results_mop <- merge(exp2_gdavg_mop_summary, exp2_rt_stats_main, by='factor')\nexp2_summary.results_fae <- merge(exp2_gdavg_fae_summary, exp2_rt_stats_interaction, by='factor') |>\n  select(-mean_high, -mean_low)\n\nexp2_summary.results <- bind_rows(exp2_summary.results_mop, exp2_summary.results_fae)\n  \nexp2_summary.results %>%\n  relocate(c(\"sd_unrelated\", \"mean.error_unrelated\"), .before=gd.mean_related) %>%\n  gt() %>%\n  cols_label(\n    CI = \"95% CI\",\n    contains(\"mean\") ~ \"mean\",\n    contains(\"sd\") ~ \"SD\", \n    contains(\"error\") ~ \"Error (%)\"\n  ) %>%\n  tab_spanner(\n    label = \"unrelated RT\",\n    columns = c(2:4)\n  ) %>%\n  tab_spanner(\n    label = \"repetition RT\",\n    columns = c(5:7)\n  ) %>%\n  tab_spanner(\n    label = 'priming effects',\n    columns = c(9:12)\n  ) %>%\n  tab_spanner(\n    label = md(\"_t_-test\"),\n    columns = c(13:15)\n  ) %>%\n  cols_label(\n    sd = md(\"SD~p~\")\n  ) %>%\n  cols_label(\n    t = md(\"_t_\"),\n    p = md(\"_p_\"),\n  ) %>%\n   sub_missing(\n    missing_text = \" \"\n  )\n```\n\n::: {.cell-output-display}\n\\begin{longtable}{lrrrrrrrrlrrrrl}\n\\toprule\n & \\multicolumn{3}{c}{unrelated RT} & \\multicolumn{3}{c}{repetition RT} &  & \\multicolumn{4}{c}{priming effects} & \\multicolumn{3}{c}{\\emph{t}-test} \\\\ \n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){9-12} \\cmidrule(lr){13-15}\nfactor & mean & SD & Error (\\%) & mean & SD & Error (\\%) & cor & MOP & 95\\% CI & SD\\textsubscript{p} & ES & \\emph{t} & df & \\emph{p} \\\\ \n\\midrule\\addlinespace[2.5pt]\nhigh & 573 & 83 & 3 & 555 & 85 & 2 & 0.860 & 18 & [16 20] & 45 & 0.41 & 19.7 & 2340 & 2.88e-80 \\\\ \nlow & 605 & 88 & 6 & 577 & 88 & 3 & 0.850 & 28 & [26 30] & 49 & 0.58 & 27.8 & 2340 & 1.52e-147 \\\\ \nnon-word & 623 & 103 & 4 & 625 & 103 & 4 & 0.910 & -2 & [-4 0] & 43 & -0.05 & -2.33 & 2340 & 0.0197 \\\\ \nfrequency:primetype &   &   &   &   &   &   & 0.029 & 10 & [7 13] & 66 & 0.15 & 7.24 & 2340 & 5.86e-13 \\\\ \n\\bottomrule\n\\end{longtable}\n\n:::\n:::\n\n\n\n\\elandscape\n\n## Discussion {#sec-exp2-discussion}\n\nExperiment 2 was specifically designed to investigate the replicability of the Frequency Attenuation Effect (FAE) observed in an unmasked environment (i.e., with a SOA > 60 ms) within the confines of a masked environment (with SOA < 60 ms). We employed a robust sample size to ensure adequate statistical power for detecting small to medium effect sizes. Our results not only replicated Experiment 1 in revealing significant main effects of repetition for high and low frequency words alike, but also detected a statistically significant interaction: the low-frequency condition manifested priming effects that were found approximately 9 ms bigger than the high-frequency condition.\n\nThe non-word masked priming response (or lack thereof) has been used as an additional piece of evidence in favor of the vision of the masked priming response as devoid of episodic influences [e.g., @Forster1999]. The results of experiment 2 align with the past evidence in showing no significant (inhibitory) masked repetition priming for non-words. However, we will not further delve into this topic here, as it does not strictly impinge on the question being asked here (i.e., word frequency effects in masked priming), and would deserve a full-fledged investigation on its own right.\n\n# General discussion {#sec-discussion}\n\nThe repetition priming response stands as a cornerstone in psycholinguistic investigations, offering insights into the mechanisms governing word recognition. An ongoing debate surrounds the interpretation of these effects, particularly concerning their source in the memory system. On the one hand, _interactive activation models_ [@McClellandRumelhart1981; @GraingerJacobs1996; @ColtheartEtal2001] posit a lexical source for repetition priming effects, either in terms of temporarily raised resting activation levels for lexical nodes in unmasked priming, or as a head start in the retrieval process in masked priming. _Episodic_ and _memory recruitment models_ [@Jacoby1981; @Jacoby1983;  @BodnerMasson1997; @MassonBodner2003; @Bodner2014] on the other hand, invoke a non-lexical source for the repetition effect, namely an episodic or episodic-like memory resource formed upon brief exposure to the prime word that can be recruited during the processing of the target item. Crucially, both models predict a single mechanism underlying masked and unmasked priming. Differential mechanisms between unmasked and masked repetition priming, however, are predicted by the _entry-opening model_ [@ForsterDavis1984], which propose both lexical and episodic sources of priming effects.\n\nThus, the existence of qualitatively distinct outcomes in masked and unmasked priming presented a direct challenge to some, but not all of these models. One such finding is the *Frequency Attenuation Effect* (FAE), in which higher frequency words exhibit smaller repetition effects compared to lower frequency words. The FAE has been described as observable only in unmasked priming since the work of @ForsterDavis1984, who demonstrated that when the prime word is presented very briefly (SOA $<$ 60 ms), it becomes masked by the target word, and this prevents the conscious encoding of the prime. Under such conditions, the FAE purportedly disappears. @ForsterDavis1984 argued that this potentially shows that the FAE is subserved by a different type of memory source (perhaps episodic) than the masked repetition priming response. This conclusion, however, is the source of ongoing debates (see @tbl-litReview for review of past findings), which the two experiments reported here were meant to address.\n\nWithin this research landscape, our experiments targeting the frequency sensitivity of the repetition effect under masked conditions contribute methodological and theoretical insights. Methodologically, our results help establish the viability and reliability of online data collection for the masked priming paradigm. Building on the pioneering work of @Angele2023 and @Cayado2023, we addressed pitfalls in implementing and analyzing masked priming data collected online, and by doing so offered a solution to the longstanding problem of low statistical power involving investigating phenomena with effect sizes that are harder to detect statistically, like interactions in factorial designs. However, this newfound opportunity necessitates careful data scrutiny, as demonstrated by significant data loss due to stringent exclusion criteria in experiments 1 and 2 (30% to 60% of the total data), highlighting the need for further exploration of less restrictive criteria and their impact on data quality.\n\nIn the same vein, the significant FAE observed in Experiment 2 has important theoretical ramifications. The historical belief in the non-observability of FAE in masked priming primarily arose from a lack of statistically significant results, possibly rooted in outdated frequency corpora or inadequate statistical power. Our design addressed these concerns, yielding statistically significant FAE results aligning with the literature's average effect (see @tbl-litReview; the 95% CI implies that the FAE is unlikely to be larger than 17 ms with a 33 ms prime duration). These results challenge the supposed qualitative distinction between masked and unmasked repetition priming cleaved by the FAE, complicating the rejection of single-mechanism theories, and suggesting that _interactive-activation models_ and _memory recruitment models_ may yet offer unifying explanations for masked and unmasked priming.\n\nSimilarly, our results also challenge the entry-opening model's prediction of the absence of FAE in masked priming. One potential way of dealing with this in the _entry opening model_ is to claim that masked priming severely reduces, but does not entirely eliminate, the use of sources other than lexical memory [see @Forster1998; @ForsterEtal2003, for proposals along this line]. Alternatively, within the entry-opening model, the results of experiment 2 may be explained by the frequency-based mechanism occurring in the fast search stage. A potential mechanism in this direction was already hinted at by @ForsterDavis1984 themselves, and consists of a procedure, whereby during the fast search stage, the entry of a prime word is promoted to the top position of the search list. As a consequence, low-frequency words (which are fairly low in the search list) will benefit from such promotion procedure more than high-frequency words (which are instead already in higher positions), thus ultimately giving rise to the FAE.\n\nWhile our findings present a compelling case for the presence of FAE in masked priming that is seemingly parallel to the unmasked case, questions about potential mechanistic differences persist. The larger sample size needed for masked FAEs raises intriguing considerations about the influence of memory sources and warrants further investigation. Additionally, the absence of significant non-word priming in experiment 2 aligns with the trend (overwhelmingly shown in the literature) that it may be exclusive to unmasked designs [@Forster1998; @ForsterEtal2003; but see @MassonBodner2003], and suggests avenues for future exploration on large-scale.\n\nFinally, the finding that the FAE occurs under masked priming conditions may impact our understanding of masked morphological priming. In this literature, there is a unresolved question about the ability of affixes to elicit masked morphological priming results [for a review, @AmentaCrepaldi2012]. In English, the evidence seems to indicate that only stems, but not affixes, have the ability to prime entries across the lexicon. This finding can and has been used to support models in which affixes are initially stripped before stems are accessed in the lexicon [@TaftForster1975; @ForsterAzuma2000; @StockallMarantz2006]. However, stems and affixes do also have a large frequency imbalance, with most affixes being substantially more frequent that most stems. The observation of FAE under masked priming can provide an alternative reason for why masked stem morphological priming is well attested but masked affix morphological priming is not: the latter could be due to a ceiling frequency attenuation effect. This is an intriguing possibility that must be left for future work to explore.\n\nIn summary, our study successfully replicated and expanded upon the work of @Angele2023 and @Cayado2023, confirming the viability of observing repetition priming effects in masked priming experiments conducted online with a brief Stimulus Onset Asynchrony (SOA) of 33 ms. Notably, we addressed a lingering question in the literature by establishing the presence of the Frequency Attenuation Effect (FAE) under masked conditions. The use of large online samples proved instrumental in overcoming the longstanding challenge of insufficient statistical power to detect interactions in factorial designs, which we believe had impeded previous investigations into detecting the FAE in masked priming.\n\nThese results not only contribute to our understanding of masked priming but also open up intriguing avenues for further research. The ability to harness extensive online samples provides a valuable opportunity to explore and illuminate unresolved issues across various domains where masked priming is a crucial research tool, underscoring the potential for online experimentation to advance our knowledge and resolve long-standing questions in the field.\n\n# References {.unnumbered}\n\n::: {#refs}\n:::\n\n\\newpage\n\n# Wordlists {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readxl)\nexp1.wordlist <- read_csv(\"materials/experiment1/frequency-effects_experiment1_word-lists_final.csv\") %>% select(-related)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 250 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (4): related, unrelated, word, condition\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nexp2.wordlist <- read_csv(\"materials/experiment2/frequency-effects_experiment2_word-lists_final.csv\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 104 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (4): related, unrelated, word, condition\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n\n### Experiment 1 {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ntrial.means <- exp1_data_final %>%\n  group_by(condition_rec, primetype_rec, target_rec, prime_rec) %>%\n  summarise(meanRT = round(mean(RT)), sdRT = round(sd(RT))) %>%\n  pivot_wider(id_cols=target_rec, names_from=primetype_rec, values_from = c(meanRT, sdRT)) %>%\n  ungroup() %>% mutate(target = tolower(target_rec)) %>% select(-target_rec) %>%\n  relocate(target, .before=meanRT_unrelated) %>%\n  relocate(sdRT_unrelated, .after=meanRT_unrelated)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'condition_rec', 'primetype_rec',\n'target_rec'. You can override using the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nexp1.wordlist %>% \n  left_join(., trial.means, by=join_by(word == target)) %>%\n  mutate(condition = ifelse(condition==\"low\", md(\"*low frequency condition*\"), \n                            ifelse(condition=='high', md(\"*high frequency condition*\"), \"non-word\"))) %>%\n  gt(groupname_col = \"condition\", process_md=T) %>%\n  tab_spanner(\n    label = \"RT (to unrelated)\",\n    columns = contains(\"RT_unrelated\")\n  ) %>%\n  tab_spanner(\n    label = \"RT (to repetition)\",\n    columns = contains(\"RT_related\")\n  ) %>%\n  cols_label(\n    unrelated = \"unrelated prime\",\n    contains(\"mean\") ~ \"mean\",\n    contains(\"sd\") ~ \"SD\"\n  ) %>%\n   sub_missing(\n    missing_text = \"--\"\n  )\n```\n\n::: {.cell-output-display}\n\\begin{longtable}{llrrrr}\n\\toprule\n &  & \\multicolumn{2}{c}{RT (to repetition)} & \\multicolumn{2}{c}{RT (to unrelated)} \\\\ \n\\cmidrule(lr){3-4} \\cmidrule(lr){5-6}\nunrelated prime & word & mean & SD & mean & SD \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{6}{l}{\\emph{low frequency condition}} \\\\ \n\\midrule\\addlinespace[2.5pt]\nsmash & chasm & 714 & 216 & 831 & 250 \\\\ \nmanna & oxide & 719 & 198 & 715 & 156 \\\\ \nlegit & vowel & 655 & 139 & 694 & 152 \\\\ \nblunt & clerk & 617 & 157 & 635 & 133 \\\\ \nslope & bleed & 609 & 171 & 621 & 176 \\\\ \nnasal & decor & 654 & 140 & 694 & 204 \\\\ \nforte & quirk & 689 & 204 & 688 & 155 \\\\ \naloud & speck & 732 & 208 & 739 & 187 \\\\ \nnymph & stash & 638 & 175 & 657 & 142 \\\\ \ncrass & ditch & 671 & 173 & 678 & 157 \\\\ \nsquid & snare & 684 & 168 & 722 & 164 \\\\ \nswirl & budge & 672 & 200 & 732 & 207 \\\\ \ngrunt & slack & 608 & 129 & 664 & 157 \\\\ \ntaunt & sedan & 711 & 197 & 705 & 122 \\\\ \ncigar & tally & 667 & 131 & 720 & 176 \\\\ \nlunge & posit & â€“ & â€“ & â€“ & â€“ \\\\ \nnegro & flock & 654 & 141 & 716 & 166 \\\\ \nexert & scorn & 670 & 159 & 651 & 146 \\\\ \nlathe & grail & 697 & 206 & 718 & 171 \\\\ \nviola & bloat & 663 & 185 & 698 & 181 \\\\ \nrival & tumor & 627 & 159 & 651 & 152 \\\\ \ndizzy & acute & 662 & 174 & 660 & 142 \\\\ \nhertz & sauna & 652 & 132 & 706 & 154 \\\\ \nhaste & elect & 640 & 162 & 650 & 144 \\\\ \npoppy & spoof & 706 & 185 & 759 & 201 \\\\ \nclove & plush & 615 & 138 & 669 & 175 \\\\ \nguise & fiend & 785 & 209 & 846 & 185 \\\\ \nmagma & knelt & 744 & 213 & 814 & 225 \\\\ \nlotto & privy & 733 & 182 & 777 & 219 \\\\ \nkayak & sigma & 798 & 258 & 796 & 205 \\\\ \ntaint & parse & â€“ & â€“ & â€“ & â€“ \\\\ \nfanny & carte & â€“ & â€“ & â€“ & â€“ \\\\ \nrouge & verge & 664 & 168 & 672 & 171 \\\\ \nvitro & mourn & 665 & 171 & 682 & 186 \\\\ \nfloss & shrug & 687 & 175 & 682 & 132 \\\\ \ntempt & clasp & 658 & 128 & 701 & 178 \\\\ \nflirt & bathe & 659 & 159 & 701 & 197 \\\\ \nfluff & linen & 620 & 91 & 650 & 133 \\\\ \nbutch & stare & 617 & 126 & 632 & 144 \\\\ \nbowel & medic & 637 & 166 & 663 & 218 \\\\ \naspen & weave & 614 & 128 & 649 & 128 \\\\ \nchime & flint & 681 & 140 & 718 & 191 \\\\ \ncrust & flank & 689 & 176 & 740 & 177 \\\\ \nspunk & scrub & 645 & 172 & 670 & 167 \\\\ \nstoke & hoist & 686 & 168 & 724 & 190 \\\\ \ndairy & stout & 667 & 148 & 707 & 166 \\\\ \nstale & cough & 588 & 147 & 629 & 157 \\\\ \ngypsy & annex & 744 & 197 & 798 & 169 \\\\ \ngloss & plume & 730 & 195 & 775 & 194 \\\\ \ntopaz & quart & 662 & 159 & 715 & 205 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{6}{l}{\\emph{high frequency condition}} \\\\ \n\\midrule\\addlinespace[2.5pt]\nshoot & proof & 576 & 119 & 617 & 129 \\\\ \nusual & clear & 598 & 169 & 588 & 120 \\\\ \nteach & audio & 589 & 141 & 632 & 112 \\\\ \nadult & apply & 592 & 154 & 632 & 130 \\\\ \nallow & phone & 573 & 143 & 588 & 89 \\\\ \nforum & class & 656 & 162 & 682 & 197 \\\\ \nwhole & raise & 611 & 154 & 598 & 116 \\\\ \noften & civil & 580 & 107 & 623 & 120 \\\\ \nissue & match & 590 & 119 & 619 & 169 \\\\ \nstyle & local & 589 & 141 & 580 & 113 \\\\ \ncoast & minor & 600 & 137 & 632 & 157 \\\\ \nreach & below & 611 & 143 & 618 & 90 \\\\ \nsmith & extra & 599 & 146 & 609 & 141 \\\\ \nspeed & court & 585 & 115 & 638 & 141 \\\\ \nsense & exact & 592 & 127 & 590 & 113 \\\\ \nwrite & bunch & 647 & 140 & 646 & 130 \\\\ \ntrust & quick & 554 & 104 & 616 & 134 \\\\ \nsleep & birth & 619 & 165 & 609 & 156 \\\\ \nreply & truth & 579 & 140 & 611 & 150 \\\\ \ntrack & serve & 611 & 136 & 649 & 168 \\\\ \ndream & trade & 606 & 185 & 602 & 106 \\\\ \nimage & heart & 592 & 159 & 602 & 113 \\\\ \nwhite & index & 606 & 111 & 625 & 146 \\\\ \nflame & cable & 583 & 119 & 626 & 130 \\\\ \nvalue & break & 605 & 163 & 601 & 133 \\\\ \navoid & woman & 576 & 119 & 609 & 153 \\\\ \nshort & front & 587 & 138 & 619 & 140 \\\\ \naware & voice & 562 & 127 & 585 & 116 \\\\ \nlarge & stock & 596 & 148 & 661 & 216 \\\\ \nprove & seven & 583 & 130 & 653 & 193 \\\\ \nbrand & blood & 568 & 109 & 598 & 109 \\\\ \nriver & plain & 596 & 115 & 617 & 123 \\\\ \nguess & solid & 643 & 158 & 612 & 140 \\\\ \nmonth & limit & 603 & 122 & 658 & 136 \\\\ \nheard & scale & 632 & 144 & 639 & 176 \\\\ \nspace & stuff & 623 & 133 & 642 & 154 \\\\ \nleave & major & 599 & 139 & 585 & 123 \\\\ \nagree & brown & 591 & 120 & 632 & 167 \\\\ \nmetal & house & 552 & 121 & 603 & 137 \\\\ \nalong & stage & 590 & 138 & 619 & 160 \\\\ \nprint & built & 628 & 155 & 664 & 166 \\\\ \nworst & video & 570 & 113 & 650 & 157 \\\\ \nsound & story & 594 & 129 & 614 & 176 \\\\ \nfaith & march & 607 & 134 & 630 & 191 \\\\ \nquote & clean & 553 & 93 & 585 & 135 \\\\ \ntrain & price & 599 & 141 & 624 & 189 \\\\ \nsmall & event & 583 & 127 & 623 & 166 \\\\ \nnight & thank & 656 & 190 & 607 & 128 \\\\ \nshell & radio & 577 & 131 & 604 & 162 \\\\ \nalone & sorry & 592 & 155 & 609 & 140 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{6}{l}{non-word} \\\\ \n\\midrule\\addlinespace[2.5pt]\nstrat & inurt & 726 & 259 & 712 & 215 \\\\ \ngleat & shawt & 760 & 270 & 672 & 154 \\\\ \ndolio & delax & 758 & 182 & 767 & 195 \\\\ \ncutch & thelp & 745 & 242 & 687 & 199 \\\\ \ngreaf & isapt & 645 & 181 & 628 & 160 \\\\ \nbroot & fopaz & 660 & 196 & 628 & 125 \\\\ \nlubic & fuxom & 676 & 234 & 601 & 126 \\\\ \ndrirk & bloot & 761 & 190 & 744 & 172 \\\\ \ncooch & scart & 768 & 220 & 726 & 162 \\\\ \nmotem & frint & 720 & 203 & 685 & 148 \\\\ \nabapt & ahuck & 673 & 207 & 633 & 153 \\\\ \nnigit & netro & 734 & 217 & 721 & 169 \\\\ \nhilac & moust & 744 & 186 & 798 & 174 \\\\ \ncojex & barsh & 731 & 216 & 706 & 183 \\\\ \nprilt & avort & 710 & 196 & 725 & 199 \\\\ \nwhirp & venem & â€“ & â€“ & â€“ & â€“ \\\\ \nshino & grack & 743 & 209 & 728 & 182 \\\\ \nnelch & ranth & 681 & 174 & 654 & 135 \\\\ \nexulk & frick & â€“ & â€“ & â€“ & â€“ \\\\ \nmorex & nohew & 683 & 197 & 656 & 165 \\\\ \ntamek & pramp & 745 & 239 & 696 & 200 \\\\ \nmiant & altep & 664 & 179 & 654 & 159 \\\\ \nbloth & scrib & 788 & 243 & 749 & 230 \\\\ \nbumbo & tumph & 785 & 204 & 768 & 210 \\\\ \noccut & dorst & 686 & 168 & 674 & 184 \\\\ \ntopec & thint & 754 & 205 & 748 & 153 \\\\ \nshoof & rourt & 691 & 192 & 688 & 194 \\\\ \nspack & smout & 759 & 195 & 736 & 184 \\\\ \nblenk & kayuk & 823 & 289 & 772 & 237 \\\\ \nsilaf & drick & 727 & 189 & 678 & 131 \\\\ \ncrunk & smoop & 710 & 185 & 684 & 154 \\\\ \nfluck & deirm & 649 & 161 & 657 & 178 \\\\ \nghisk & ephic & 787 & 223 & 751 & 212 \\\\ \nchrik & glurp & 731 & 209 & 727 & 236 \\\\ \ncetup & blumb & 746 & 183 & 733 & 220 \\\\ \nfirch & eicht & 725 & 226 & 718 & 205 \\\\ \nvasem & forim & 736 & 214 & 690 & 185 \\\\ \nearch & slent & 840 & 207 & 773 & 178 \\\\ \nblont & lepot & 693 & 203 & 659 & 162 \\\\ \necret & plock & 763 & 222 & 734 & 195 \\\\ \nwateb & ocheb & 643 & 168 & 620 & 130 \\\\ \ntrook & febut & 659 & 166 & 632 & 156 \\\\ \nruzak & coreb & 656 & 169 & 643 & 133 \\\\ \ntheet & frath & 738 & 193 & 699 & 148 \\\\ \nblamp & eggem & 705 & 190 & 681 & 160 \\\\ \nlambo & gredo & 700 & 217 & 689 & 182 \\\\ \naliom & brost & 728 & 204 & 690 & 170 \\\\ \nbrust & ganic & 712 & 178 & 660 & 117 \\\\ \ncleot & polep & 714 & 236 & 641 & 174 \\\\ \nlindo & snock & 766 & 194 & 776 & 206 \\\\ \ndriff & fomit & 711 & 187 & 633 & 147 \\\\ \nwrast & sholf & 665 & 157 & 642 & 111 \\\\ \nlidst & racef & 668 & 167 & 658 & 171 \\\\ \nhuirk & thamp & 711 & 188 & 708 & 226 \\\\ \npumbo & purso & 702 & 196 & 665 & 168 \\\\ \nwhilo & glarm & 765 & 210 & 748 & 184 \\\\ \nmurkt & fingo & 707 & 164 & 683 & 179 \\\\ \nsteck & gotch & â€“ & â€“ & â€“ & â€“ \\\\ \nmolax & spuff & 745 & 198 & 692 & 151 \\\\ \nronch & schew & 811 & 294 & 756 & 265 \\\\ \nguesh & humot & 690 & 175 & 674 & 149 \\\\ \nsnump & sgrew & 706 & 212 & 724 & 175 \\\\ \nfleak & fadio & 713 & 175 & 678 & 141 \\\\ \nrecup & plint & 768 & 246 & 735 & 225 \\\\ \nloast & pheek & 696 & 181 & 676 & 192 \\\\ \nsmalt & blasm & 785 & 226 & 755 & 175 \\\\ \nswimp & reash & 780 & 187 & 754 & 181 \\\\ \ntymph & chank & 798 & 229 & 774 & 221 \\\\ \nlaget & septh & 721 & 196 & 688 & 193 \\\\ \ngluck & feeth & 756 & 191 & 720 & 156 \\\\ \ngatob & tosit & 683 & 184 & 668 & 210 \\\\ \nsauto & exuct & 767 & 232 & 693 & 191 \\\\ \ncrunt & ethym & 724 & 211 & 700 & 213 \\\\ \npranc & feght & 718 & 187 & 723 & 203 \\\\ \ntwank & stoff & 709 & 165 & 688 & 155 \\\\ \nletap & cruck & 742 & 197 & 812 & 229 \\\\ \nalash & fatho & 643 & 146 & 660 & 184 \\\\ \nsharf & firsh & 717 & 168 & 717 & 196 \\\\ \nfrimp & paltz & 688 & 211 & 719 & 227 \\\\ \nlumpo & thark & 683 & 134 & 714 & 205 \\\\ \nhuilt & aufit & 638 & 146 & 649 & 184 \\\\ \nbrosk & hinup & 636 & 126 & 653 & 142 \\\\ \ndulch & jongo & 681 & 181 & 705 & 202 \\\\ \ndealf & guast & 670 & 178 & 687 & 210 \\\\ \ndrash & sunch & 697 & 196 & 692 & 190 \\\\ \nprock & cleak & 766 & 177 & 819 & 214 \\\\ \nspaft & stram & 720 & 157 & 726 & 155 \\\\ \ncriex & etuip & 620 & 138 & 635 & 177 \\\\ \nphumb & opert & 750 & 225 & 791 & 255 \\\\ \ndenet & keach & 670 & 176 & 700 & 189 \\\\ \nbluck & umarm & 719 & 213 & 756 & 239 \\\\ \nracet & tooch & 739 & 213 & 741 & 234 \\\\ \nphrap & chuth & 682 & 152 & 726 & 208 \\\\ \nwight & tedic & 695 & 196 & 704 & 199 \\\\ \nlorro & mutch & 796 & 257 & 811 & 279 \\\\ \noorph & hilth & 682 & 195 & 711 & 213 \\\\ \npraph & pluff & â€“ & â€“ & â€“ & â€“ \\\\ \naboot & widet & 799 & 222 & 818 & 251 \\\\ \nhoest & scook & 721 & 168 & 749 & 201 \\\\ \npolic & fisco & 797 & 261 & 797 & 271 \\\\ \nglunk & gamit & 751 & 257 & 725 & 243 \\\\ \nletch & phasm & â€“ & â€“ & â€“ & â€“ \\\\ \nspink & sondo & 679 & 168 & 672 & 182 \\\\ \ndippo & vuint & 634 & 137 & 616 & 130 \\\\ \nastef & rynic & 629 & 123 & 658 & 161 \\\\ \ntatch & waget & 736 & 205 & 747 & 211 \\\\ \nshoop & vooch & 671 & 158 & 691 & 169 \\\\ \nisloo & guilm & 675 & 179 & 719 & 210 \\\\ \nscack & elsom & 686 & 195 & 704 & 248 \\\\ \nbliff & crost & 718 & 190 & 731 & 199 \\\\ \ncempo & alept & 754 & 186 & 780 & 216 \\\\ \nglaim & robit & 741 & 206 & 783 & 220 \\\\ \nthunt & noast & 658 & 122 & 688 & 160 \\\\ \nplesh & bealm & 740 & 175 & 759 & 204 \\\\ \nthoop & hyrup & 703 & 125 & 741 & 191 \\\\ \nlouth & chost & 752 & 192 & 778 & 209 \\\\ \npreak & borif & 617 & 111 & 616 & 130 \\\\ \ncreck & starp & 751 & 215 & 744 & 208 \\\\ \nrealp & valif & 656 & 178 & 678 & 190 \\\\ \nferit & raceb & 674 & 203 & 687 & 174 \\\\ \ntheep & dacit & 642 & 171 & 649 & 179 \\\\ \nmurch & abert & 733 & 190 & 765 & 233 \\\\ \nblomp & paith & 703 & 161 & 724 & 170 \\\\ \nsloup & mough & 710 & 145 & 719 & 145 \\\\ \nstrit & plick & 768 & 218 & 793 & 232 \\\\ \nskinp & toost & 763 & 198 & 786 & 218 \\\\ \nphock & tacao & 751 & 217 & 778 & 285 \\\\ \ncyrrh & kneak & 790 & 212 & 826 & 228 \\\\ \nahack & vitch & 682 & 155 & 717 & 238 \\\\ \nsaist & paxim & 647 & 152 & 667 & 185 \\\\ \npheep & kingo & 734 & 167 & 738 & 175 \\\\ \nehert & truff & 767 & 218 & 771 & 246 \\\\ \nspuck & fundt & 655 & 162 & 700 & 183 \\\\ \nantuc & bloam & 719 & 155 & 741 & 191 \\\\ \nshish & quilp & 726 & 217 & 718 & 233 \\\\ \ngijou & fotch & 658 & 127 & 661 & 132 \\\\ \ndrarp & broup & 674 & 161 & 690 & 213 \\\\ \nstilp & krauf & 683 & 183 & 683 & 200 \\\\ \ndoint & swaft & 826 & 286 & 821 & 253 \\\\ \nowlut & adoof & 726 & 176 & 724 & 191 \\\\ \nswant & meash & 722 & 195 & 776 & 230 \\\\ \nvepot & afent & 660 & 151 & 651 & 180 \\\\ \nploic & setip & 705 & 198 & 710 & 203 \\\\ \nglick & linew & 769 & 207 & 794 & 242 \\\\ \nhatex & corax & 696 & 162 & 755 & 218 \\\\ \nframo & scock & 811 & 233 & 807 & 232 \\\\ \npraft & quast & 733 & 193 & 763 & 211 \\\\ \nminch & ipept & 685 & 201 & 691 & 209 \\\\ \nragic & gonet & 658 & 172 & 692 & 203 \\\\ \nstabt & lertz & 629 & 153 & 652 & 155 \\\\ \n\\bottomrule\n\\end{longtable}\n\n:::\n:::\n\n\n\n\n### Experiment 2 {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ntrial.means_exp2 <- exp2_data_final %>%\n  group_by(condition_rec, primetype_rec, target_rec, prime_rec) %>%\n  summarise(meanRT = round(mean(RT)), sdRT = round(sd(RT))) %>%\n  pivot_wider(id_cols=target_rec, names_from=primetype_rec, values_from = c(meanRT, sdRT)) %>%\n  ungroup() %>% mutate(target = tolower(target_rec)) %>% select(-target_rec) %>%\n  relocate(target, .before=meanRT_unrelated) %>%\n  relocate(sdRT_unrelated, .after=meanRT_unrelated)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'condition_rec', 'primetype_rec',\n'target_rec'. You can override using the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nexp2.wordlist %>% \n  left_join(., trial.means_exp2, by=join_by(word == target)) %>%\n  mutate(condition = ifelse(condition==\"low\", md(\"*low frequency condition*\"), \n                            ifelse(condition=='high', md(\"*high frequency condition*\"), \"non-word\"))) %>%\n  gt(groupname_col = \"condition\", process_md=T) %>%\n  tab_spanner(\n    label = \"RT (to unrelated)\",\n    columns = contains(\"RT_unrelated\")\n  ) %>%\n  tab_spanner(\n    label = \"RT (to repetition)\",\n    columns = contains(\"RT_related\")\n  ) %>%\n  cols_label(\n    unrelated = \"unrelated prime\",\n    contains(\"mean\") ~ \"mean\",\n    contains(\"sd\") ~ \"SD\"\n  ) %>%\n   sub_missing(\n    missing_text = \"--\"\n  )\n```\n\n::: {.cell-output-display}\n\\begin{longtable}{lllrrrr}\n\\toprule\n &  &  & \\multicolumn{2}{c}{RT (to repetition)} & \\multicolumn{2}{c}{RT (to unrelated)} \\\\ \n\\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\nrelated & unrelated prime & word & mean & SD & mean & SD \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{7}{l}{\\emph{low frequency condition}} \\\\ \n\\midrule\\addlinespace[2.5pt]\narrow & hunch & arrow & 590 & 130 & 587 & 124 \\\\ \npitch & sneak & pitch & 576 & 126 & 612 & 122 \\\\ \nhatch & widow & hatch & 621 & 151 & 639 & 148 \\\\ \nshark & brief & shark & 573 & 125 & 590 & 138 \\\\ \ntooth & sharp & tooth & 536 & 125 & 565 & 116 \\\\ \nbooth & grief & booth & 572 & 136 & 627 & 157 \\\\ \npound & sting & pound & 551 & 127 & 572 & 127 \\\\ \nweigh & thief & weigh & 593 & 167 & 636 & 164 \\\\ \nblank & avoid & blank & 571 & 139 & 596 & 124 \\\\ \ncrush & award & crush & 554 & 128 & 592 & 136 \\\\ \nbench & smack & bench & 573 & 132 & 601 & 129 \\\\ \nfetch & brand & fetch & 622 & 156 & 658 & 146 \\\\ \ncheek & salad & cheek & 561 & 141 & 602 & 142 \\\\ \nbrush & swamp & brush & 564 & 130 & 600 & 128 \\\\ \nmarch & depth & march & 559 & 125 & 580 & 123 \\\\ \nbleed & flesh & bleed & 560 & 148 & 577 & 146 \\\\ \ncliff & harsh & cliff & 602 & 130 & 645 & 137 \\\\ \nfraud & creep & fraud & 621 & 147 & 628 & 132 \\\\ \ncloud & plead & cloud & 536 & 115 & 551 & 101 \\\\ \nfluid & thumb & fluid & 605 & 140 & 678 & 162 \\\\ \ntrash & creek & trash & 554 & 127 & 560 & 128 \\\\ \nflush & blond & flush & 576 & 123 & 617 & 140 \\\\ \nporch & stink & porch & 587 & 136 & 620 & 160 \\\\ \nstiff & patch & stiff & 626 & 154 & 678 & 156 \\\\ \ncough & sweep & cough & 564 & 142 & 601 & 141 \\\\ \nsmash & squad & smash & 570 & 129 & 587 & 126 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{7}{l}{\\emph{high frequency condition}} \\\\ \n\\midrule\\addlinespace[2.5pt]\nblood & chief & blood & 541 & 130 & 551 & 104 \\\\ \nbunch & child & bunch & 585 & 148 & 617 & 145 \\\\ \ncatch & board & catch & 545 & 116 & 562 & 130 \\\\ \nstuff & tough & stuff & 555 & 119 & 585 & 137 \\\\ \nbreak & stand & break & 545 & 107 & 561 & 124 \\\\ \nspeak & beach & speak & 545 & 131 & 573 & 129 \\\\ \nstick & hotel & stick & 562 & 128 & 598 & 138 \\\\ \nsleep & angel & sleep & 538 & 113 & 559 & 119 \\\\ \nwrong & truth & wrong & 563 & 143 & 565 & 132 \\\\ \ngrand & quick & grand & 571 & 127 & 582 & 143 \\\\ \nmouth & world & mouth & 543 & 125 & 556 & 119 \\\\ \nknock & extra & knock & 560 & 134 & 631 & 136 \\\\ \nguard & think & guard & 580 & 132 & 590 & 134 \\\\ \nsmall & thing & small & 557 & 130 & 577 & 125 \\\\ \ncheck & round & check & 558 & 135 & 562 & 121 \\\\ \nwatch & proud & watch & 541 & 128 & 546 & 110 \\\\ \ngroup & smell & group & 559 & 127 & 576 & 142 \\\\ \nmonth & earth & month & 555 & 120 & 572 & 123 \\\\ \nsouth & relax & south & 575 & 139 & 611 & 133 \\\\ \nlunch & truck & lunch & 547 & 119 & 557 & 125 \\\\ \nclock & throw & clock & 548 & 132 & 574 & 124 \\\\ \nsound & death & sound & 538 & 127 & 552 & 103 \\\\ \ndrink & north & drink & 559 & 129 & 556 & 122 \\\\ \ntouch & young & touch & 541 & 122 & 573 & 121 \\\\ \nlaugh & weird & laugh & 546 & 119 & 568 & 121 \\\\ \nblack & reach & black & 553 & 131 & 563 & 114 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{7}{l}{non-word} \\\\ \n\\midrule\\addlinespace[2.5pt]\nalkew & grack & alkew & 599 & 153 & 591 & 140 \\\\ \nagink & furob & agink & 626 & 148 & 614 & 141 \\\\ \nruzak & begro & ruzak & 577 & 130 & 584 & 142 \\\\ \nsondo & labok & sondo & 625 & 142 & 612 & 149 \\\\ \nguesh & gazzo & guesh & 702 & 184 & 721 & 194 \\\\ \nfadio & criam & fadio & 618 & 149 & 604 & 146 \\\\ \nplich & coreb & plich & 650 & 162 & 640 & 159 \\\\ \nsgrew & docab & sgrew & 626 & 182 & 638 & 182 \\\\ \nsceak & colob & sceak & 675 & 154 & 683 & 171 \\\\ \nghisk & isloo & ghisk & 588 & 139 & 593 & 139 \\\\ \ndeirm & ahuck & deirm & 589 & 142 & 596 & 139 \\\\ \nvillo & flurb & villo & 632 & 182 & 615 & 181 \\\\ \ntidow & pikto & tidow & 648 & 167 & 624 & 160 \\\\ \ndrick & aliom & drick & 684 & 168 & 681 & 172 \\\\ \nphick & purso & phick & 643 & 160 & 637 & 165 \\\\ \nnello & borno & nello & 625 & 156 & 612 & 151 \\\\ \nfeach & pacaw & feach & 730 & 201 & 720 & 192 \\\\ \ntello & rilth & tello & 651 & 175 & 644 & 171 \\\\ \ndolio & caveb & dolio & 602 & 148 & 610 & 165 \\\\ \ngorgo & swysh & gorgo & 643 & 164 & 619 & 170 \\\\ \nwhilo & lanjo & whilo & 612 & 137 & 604 & 150 \\\\ \nstanf & drief & stanf & 611 & 134 & 617 & 133 \\\\ \ncrulk & ocheb & crulk & 671 & 162 & 665 & 169 \\\\ \nphumb & tunch & phumb & 645 & 160 & 633 & 148 \\\\ \nsirth & steaf & sirth & 612 & 141 & 618 & 145 \\\\ \nslerk & nohew & slerk & 640 & 153 & 634 & 163 \\\\ \nvitbo & nualm & vitbo & 593 & 151 & 596 & 154 \\\\ \nsunch & ofium & sunch & 665 & 165 & 665 & 161 \\\\ \nsoeth & croik & soeth & 589 & 141 & 589 & 130 \\\\ \neltow & valuo & eltow & 628 & 171 & 606 & 158 \\\\ \nframo & sorgo & framo & 617 & 146 & 618 & 146 \\\\ \nlumpo & shavo & lumpo & 630 & 162 & 635 & 172 \\\\ \nspuff & oceab & spuff & 672 & 169 & 667 & 183 \\\\ \ngatob & tolio & gatob & 599 & 139 & 606 & 155 \\\\ \nnosom & theck & nosom & 598 & 155 & 604 & 139 \\\\ \ngezzo & tooch & gezzo & 592 & 136 & 586 & 131 \\\\ \nafoub & slonk & afoub & 582 & 133 & 589 & 128 \\\\ \nwateb & salch & wateb & 633 & 151 & 619 & 133 \\\\ \nnelch & raceb & nelch & 601 & 144 & 594 & 145 \\\\ \ndahoo & ahack & dahoo & 598 & 132 & 595 & 146 \\\\ \ndriek & fideo & driek & 606 & 145 & 606 & 143 \\\\ \ngnask & fluko & gnask & 612 & 171 & 604 & 153 \\\\ \nbrosk & cyrrh & brosk & 629 & 159 & 647 & 175 \\\\ \nduvez & revuo & duvez & 580 & 152 & 580 & 155 \\\\ \nfielm & cempo & fielm & 609 & 146 & 611 & 151 \\\\ \npumph & exulk & pumph & 669 & 162 & 685 & 176 \\\\ \ngerif & kleck & gerif & 584 & 137 & 588 & 149 \\\\ \nracef & bonth & racef & 618 & 151 & 622 & 156 \\\\ \npheek & scook & pheek & 640 & 155 & 644 & 176 \\\\ \npruaw & slork & pruaw & 593 & 133 & 592 & 135 \\\\ \nguilm & whilf & guilm & 603 & 142 & 598 & 142 \\\\ \nlairf & drosh & lairf & 587 & 144 & 600 & 150 \\\\ \n\\bottomrule\n\\end{longtable}\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{caption}\n\\usepackage{longtable}\n\\usepackage{colortbl}\n\\usepackage{array}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}